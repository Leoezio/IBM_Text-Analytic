{
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# This notebok demonstrates how to use python NLTK package for text cleaning and text preparation. It also shows how to perform cosine similarity to find similar documents.\n\n## 2017 Dec Shilpa Jain", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Install Python NLTK package\n\nNLTK is a natural language toolkit for building programs in Python that work with natural language text.\nWe will use NLTK for this course.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting nltk\n  Downloading nltk-3.2.5.tar.gz (1.2MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.2MB 827kB/s eta 0:00:01\n\u001b[?25hCollecting six (from nltk)\n  Downloading six-1.11.0-py2.py3-none-any.whl\nBuilding wheels for collected packages: nltk\n  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/.cache/pip/wheels/18/9c/1f/276bc3f421614062468cb1c9d695e6086d0c73d67ea363c501\nSuccessfully built nltk\nInstalling collected packages: six, nltk\nSuccessfully installed nltk-3.2.5 six-1.11.0\n"
                }
            ], 
            "cell_type": "code", 
            "source": "!pip install nltk --upgrade"
        }, 
        {
            "source": "## Import NLTK and download NLTK book collection", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "NLTK Downloader\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> d\n\nDownload which package (l=list; x=cancel)?\n  Identifier> l\nPackages:\n  [ ] abc................. Australian Broadcasting Commission 2006\n  [ ] alpino.............. Alpino Dutch Treebank\n  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n  [ ] basque_grammars..... Grammars for Basque\n  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n                           Extraction Systems in Biology)\n  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n  [ ] book_grammars....... Grammars from NLTK Book\n  [ ] brown............... Brown Corpus\n  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n  [ ] cess_cat............ CESS-CAT Treebank\n  [ ] cess_esp............ CESS-ESP Treebank\n  [ ] chat80.............. Chat-80 Data Files\n  [ ] city_database....... City Database\n  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n  [ ] comparative_sentences Comparative Sentence Dataset\n  [ ] comtrans............ ComTrans Corpus Sample\n  [ ] conll2000........... CONLL 2000 Chunking Corpus\n  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\nHit Enter to continue: book\n  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n                           and Basque Subset)\n  [ ] crubadan............ Crubadan Corpus\n  [ ] dependency_treebank. Dependency Parsed Treebank\n  [ ] dolch............... Dolch Word List\n  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n                           Corpus\n  [ ] floresta............ Portuguese Treebank\n  [ ] framenet_v15........ FrameNet 1.5\n  [ ] framenet_v17........ FrameNet 1.7\n  [ ] gazetteers.......... Gazeteer Lists\n  [ ] genesis............. Genesis Corpus\n  [ ] gutenberg........... Project Gutenberg Selections\n  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n  [ ] ieer................ NIST IE-ER DATA SAMPLE\n  [ ] inaugural........... C-Span Inaugural Address Corpus\n  [ ] indian.............. Indian Language POS-Tagged Corpus\n  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n                           ChaSen format)\n  [ ] kimmo............... PC-KIMMO Data Files\n  [ ] knbc................ KNB Corpus (Annotated blog corpus)\nHit Enter to continue: \n  [ ] large_grammars...... Large context-free and feature-based grammars\n                           for parser comparison\n  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n                           part-of-speech tags\n  [ ] machado............. Machado de Assis -- Obra Completa\n  [ ] masc_tagged......... MASC Tagged Corpus\n  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n  [ ] moses_sample........ Moses Sample Models\n  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n                           2015) subset of the Paraphrase Database.\n  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n  [ ] nombank.1.0......... NomBank Corpus 1.0\n  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n  [ ] nps_chat............ NPS Chat\n  [ ] omw................. Open Multilingual Wordnet\n  [ ] opinion_lexicon..... Opinion Lexicon\n  [ ] panlex_swadesh...... PanLex Swadesh Corpora\nHit Enter to continue: \n  [ ] paradigms........... Paradigm Corpus\n  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n                           Evaluation Shared Task\n  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n                           character properties in Perl\n  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n  [ ] pl196x.............. Polish language of the XX century sixties\n  [ ] porter_test......... Porter Stemmer Test Files\n  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n  [ ] problem_reports..... Problem Report Corpus\n  [ ] product_reviews_1... Product Reviews (5 Products)\n  [ ] product_reviews_2... Product Reviews (9 Products)\n  [ ] propbank............ Proposition Bank Corpus 1.0\n  [ ] pros_cons........... Pros and Cons\n  [ ] ptb................. Penn Treebank\n  [ ] punkt............... Punkt Tokenizer Models\n  [ ] qc.................. Experimental Data for Question Classification\n  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n                           version\n  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n                           Portuguesa)\nHit Enter to continue: \n  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n  [ ] sample_grammars..... Sample Grammars\n  [ ] semcor.............. SemCor 3.0\n  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n  [ ] sentiwordnet........ SentiWordNet\n  [ ] shakespeare......... Shakespeare XML Corpus Sample\n  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n  [ ] smultron............ SMULTRON Corpus Sample\n  [ ] snowball_data....... Snowball Data\n  [ ] spanish_grammars.... Grammars for Spanish\n  [ ] state_union......... C-Span State of the Union Address Corpus\n  [ ] stopwords........... Stopwords Corpus\n  [ ] subjectivity........ Subjectivity Dataset v1.0\n  [ ] swadesh............. Swadesh Wordlists\n  [ ] switchboard......... Switchboard Corpus Sample\n  [ ] tagsets............. Help on Tagsets\n  [ ] timit............... TIMIT Corpus Sample\n  [ ] toolbox............. Toolbox Sample Files\n  [ ] treebank............ Penn Treebank Sample\n  [ ] twitter_samples..... Twitter Samples\nHit Enter to continue: \n  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n                           (Unicode Version)\n  [ ] udhr................ Universal Declaration of Human Rights Corpus\n  [ ] unicode_samples..... Unicode Samples\n  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n  [ ] vader_lexicon....... VADER Sentiment Lexicon\n  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n  [ ] webtext............. Web Text Corpus\n  [ ] wmt15_eval.......... Evaluation data from WMT15\n  [ ] word2vec_sample..... Word2Vec Sample\n  [ ] wordnet............. WordNet\n  [ ] wordnet_ic.......... WordNet-InfoContent\n  [ ] words............... Word Lists\n  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n                           English Prose\n\nCollections:\n  [ ] all-corpora......... All the corpora\n  [ ] all-nltk............ All packages available on nltk_data gh-pages\n                           branch\n  [ ] all................. All packages\nHit Enter to continue: \n  [ ] book................ Everything used in the NLTK Book\n  [ ] popular............. Popular packages\n  [ ] third-party......... Third-party data packages\n\n([*] marks installed packages)\n\nDownload which package (l=list; x=cancel)?\n  Identifier> \n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> d\n\nDownload which package (l=list; x=cancel)?\n  Identifier> book\n    Downloading collection 'book'\n       | \n       | Downloading package abc to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/abc.zip.\n       | Downloading package brown to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/brown.zip.\n       | Downloading package chat80 to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/chat80.zip.\n       | Downloading package cmudict to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/cmudict.zip.\n       | Downloading package conll2000 to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/conll2000.zip.\n       | Downloading package conll2002 to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/conll2002.zip.\n       | Downloading package dependency_treebank to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/dependency_treebank.zip.\n       | Downloading package genesis to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/genesis.zip.\n       | Downloading package gutenberg to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/gutenberg.zip.\n       | Downloading package ieer to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/ieer.zip.\n       | Downloading package inaugural to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/inaugural.zip.\n       | Downloading package movie_reviews to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/movie_reviews.zip.\n       | Downloading package nps_chat to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/nps_chat.zip.\n       | Downloading package names to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/names.zip.\n       | Downloading package ppattach to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/ppattach.zip.\n       | Downloading package reuters to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       | Downloading package senseval to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/senseval.zip.\n       | Downloading package state_union to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/state_union.zip.\n       | Downloading package stopwords to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/stopwords.zip.\n       | Downloading package swadesh to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/swadesh.zip.\n       | Downloading package timit to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/timit.zip.\n       | Downloading package treebank to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/treebank.zip.\n       | Downloading package toolbox to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/toolbox.zip.\n       | Downloading package udhr to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/udhr.zip.\n       | Downloading package udhr2 to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/udhr2.zip.\n       | Downloading package unicode_samples to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/unicode_samples.zip.\n       | Downloading package webtext to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/webtext.zip.\n       | Downloading package wordnet to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/wordnet.zip.\n       | Downloading package wordnet_ic to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/wordnet_ic.zip.\n       | Downloading package words to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/words.zip.\n       | Downloading package maxent_treebank_pos_tagger to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n       | Downloading package maxent_ne_chunker to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping chunkers/maxent_ne_chunker.zip.\n       | Downloading package universal_tagset to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping taggers/universal_tagset.zip.\n       | Downloading package punkt to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping tokenizers/punkt.zip.\n       | Downloading package book_grammars to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping grammars/book_grammars.zip.\n       | Downloading package city_database to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping corpora/city_database.zip.\n       | Downloading package tagsets to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping help/tagsets.zip.\n       | Downloading package panlex_swadesh to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       | Downloading package averaged_perceptron_tagger to\n       |     /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/nltk_dat\n       |     a...\n       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n       | \n     Done downloading collection book\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> q\n*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\ntext4: Inaugural Address Corpus\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
                }
            ], 
            "cell_type": "code", 
            "source": "import nltk\nnltk.download()\n"
        }, 
        {
            "source": "## Cell below will load all the items in the book module that you have just downloaded. When this finishes, we will see the output.\nWe can see from the output that there are 9 pieces of text and 9 sentences loaded. For example, if we\ntype text1, we will see the title of the first piece of text. If we type sent3, we will see the body of the\nthird sentence.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\ntext4: Inaugural Address Corpus\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
                }
            ], 
            "cell_type": "code", 
            "source": "from nltk.book import *"
        }, 
        {
            "execution_count": 125, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "<Text: Moby Dick by Herman Melville 1851>"
                    }, 
                    "execution_count": 125, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "(text1)"
        }, 
        {
            "execution_count": 8, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "['In',\n 'the',\n 'beginning',\n 'God',\n 'created',\n 'the',\n 'heaven',\n 'and',\n 'the',\n 'earth',\n '.']"
                    }, 
                    "execution_count": 8, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "sent3"
        }, 
        {
            "source": "##### In NLTK, there is a method called concordance that allows us to search for a word inside a piece of text.\n##### Count method returns the number of times a word occurs in a piece of text.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Displaying 12 of 12 matches:\n of the brain .\" -- ULLOA ' S SOUTH AMERICA . \" To fifty chosen sylphs of speci\n, in spite of this , nowhere in all America will you find more patrician - like\nhree pirate powers did Poland . Let America add Mexico to Texas , and pile Cuba\n , how comes it that we whalemen of America now outnumber all the rest of the b\nmocracy in those parts . That great America on the other side of the sphere , A\nf age ; though among the Red Men of America the giving of the white belt of wam\n and fifty leagues from the Main of America , our ship felt a terrible shock , \n, in the land - locked heart of our America , had yet been nurtured by all thos\n some Nor ' West Indian long before America was discovered . What other marvels\nd universally applicable . What was America in 1492 but a Loose - Fish , in whi\nw those noble golden coins of South America are as medals of the sun and tropic\nod of the last one must be grown in America .\" \" Aye , aye ! a strange sight th\n"
                }, 
                {
                    "data": {
                        "text/plain": "11"
                    }, 
                    "execution_count": 15, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "text1.concordance(\"America\")\ntext1.count(\"America\")"
        }, 
        {
            "source": "## UsingWord Counts to Obtain an Overview of a Collection\nAssume that you have a large document collection. For example, it could be all the email enquiries\nfrom the customers of a company in a particular month. It could be all the tweets published by a particular\nuser. It could also be all the fictions written by a particular author. Without going through all the documents\ninside the collection, how can you quickly get an idea about the major topics or themes covered by these\ndocuments?\n\nIn NLTK, there is a built-in function called FreqDist() that makes our task very easy.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "FreqDist({'permit': 1,\n          'household': 3,\n          'circle': 2,\n          'instantaneously': 1,\n          'objections': 7,\n          'afford': 15,\n          'weeks': 23,\n          'disagreeable': 2,\n          'gained': 15,\n          'anticipated': 2,\n          'concealment': 5,\n          'Can': 7,\n          'spoken': 15,\n          'thirteen': 1,\n          'encouraged': 5,\n          'any': 389,\n          'brief': 6,\n          'embarrassment': 9,\n          'degree': 14,\n          'contented': 4,\n          'bind': 1,\n          'fall': 6,\n          'why': 25,\n          'affliction': 16,\n          'preceding': 6,\n          'friends': 62,\n          'trembled': 2,\n          'luckily': 6,\n          'fertile': 1,\n          'Hanover': 2,\n          'furniture': 10,\n          'Jenning': 5,\n          'debated': 1,\n          'did': 211,\n          'able': 46,\n          'regain': 1,\n          'wild': 4,\n          'arranged': 8,\n          'avoided': 7,\n          'overspreading': 1,\n          'Sharpe': 1,\n          'absent': 3,\n          'politics': 1,\n          'venture': 8,\n          'reception': 6,\n          'About': 9,\n          'cautiously': 1,\n          ']': 1,\n          'dread': 7,\n          'improvements': 7,\n          'mantelpiece': 1,\n          'chuckle': 1,\n          'blights': 1,\n          'sooth': 1,\n          'her': 2436,\n          'fuss': 1,\n          'senses': 4,\n          'design': 10,\n          'yourselves': 3,\n          'sporting': 2,\n          'believe': 89,\n          'mistakes': 1,\n          'reasons': 6,\n          'compass': 1,\n          'resist': 9,\n          'general': 55,\n          'got': 27,\n          'persuaded': 16,\n          'unseen': 1,\n          'vowed': 1,\n          'formed': 23,\n          'blockhead': 1,\n          'roof': 3,\n          'pull': 1,\n          'included': 1,\n          'dies': 1,\n          'plea': 1,\n          'parcels': 1,\n          'ponds': 1,\n          'disposing': 1,\n          'elevated': 4,\n          'explained': 11,\n          'knack': 1,\n          'instant': 8,\n          '47': 1,\n          'illustration': 1,\n          'judging': 3,\n          'abroad': 9,\n          'so': 617,\n          'looks': 17,\n          'rather': 73,\n          'casement': 1,\n          '28': 1,\n          'heard': 78,\n          'clapped': 1,\n          'reconciled': 4,\n          'seemed': 91,\n          'tiled': 1,\n          'excited': 6,\n          'up': 111,\n          'attentions': 8,\n          'Lady': 99,\n          'fetching': 2,\n          'TWO': 1,\n          'chariot': 2,\n          'politeness': 14,\n          'ambition': 2,\n          'ample': 1,\n          'while': 107,\n          'clothes': 4,\n          'alteration': 15,\n          'permitted': 2,\n          'solicitation': 1,\n          'generously': 1,\n          'prejudiced': 2,\n          'dressing': 5,\n          'thunderbolt': 1,\n          'satisfied': 26,\n          'refreshment': 1,\n          'deceived': 11,\n          'Charlotte': 24,\n          'Supposing': 1,\n          'crowd': 3,\n          'carries': 2,\n          'You': 128,\n          'steward': 1,\n          'efficacy': 1,\n          'Soon': 1,\n          'covenant': 1,\n          'narrow': 5,\n          'intermission': 3,\n          'bell': 2,\n          'tongue': 2,\n          'blushed': 5,\n          'affair': 28,\n          'violins': 1,\n          'transacted': 1,\n          'romance': 1,\n          '5': 1,\n          'come': 95,\n          'scorn': 1,\n          'supplied': 3,\n          'attributed': 4,\n          'erred': 3,\n          'procure': 4,\n          'separate': 4,\n          'unsteady': 1,\n          'riding': 1,\n          'lady': 43,\n          'eloping': 1,\n          'extort': 1,\n          'inheritor': 1,\n          'sealed': 1,\n          'lines': 3,\n          'Eager': 1,\n          'need': 17,\n          'Austen': 1,\n          'unfashionable': 1,\n          'pitched': 1,\n          'genuine': 4,\n          'coolness': 3,\n          'delight': 24,\n          'accept': 10,\n          'she': 1333,\n          'us': 89,\n          'banker': 1,\n          'whiled': 1,\n          'spoiling': 1,\n          'older': 1,\n          'suitor': 1,\n          'Street': 45,\n          'However': 10,\n          'Sensibility': 1,\n          'disinclination': 2,\n          'expects': 2,\n          'nearly': 8,\n          'hither': 1,\n          'valueless': 1,\n          'strictest': 2,\n          '.)': 2,\n          'significancy': 1,\n          'cottages': 2,\n          'facts': 1,\n          'Engagement': 1,\n          'unremitting': 1,\n          'Last': 2,\n          'tax': 1,\n          'regarding': 3,\n          'defects': 2,\n          'mortifying': 2,\n          'feet': 3,\n          'Pleased': 1,\n          'Add': 1,\n          'an': 346,\n          'unintelligible': 1,\n          'five': 31,\n          'Sandersons': 1,\n          'smirked': 1,\n          'persons': 1,\n          'profess': 3,\n          'firmer': 2,\n          'henceforward': 1,\n          'tower': 1,\n          'Holburn': 1,\n          'forgive': 8,\n          'ordered': 2,\n          'Disappointed': 1,\n          'disputes': 1,\n          'sorrows': 6,\n          'feature': 1,\n          'bidding': 1,\n          'uniform': 1,\n          'disapproving': 1,\n          'Abbeyland': 1,\n          'phrase': 2,\n          'gratitude': 18,\n          'remedy': 1,\n          'recollecting': 4,\n          'dispute': 1,\n          'sweep': 1,\n          'converse': 3,\n          'strict': 2,\n          'mechanically': 1,\n          'editions': 1,\n          'sashes': 1,\n          'witnesses': 1,\n          'uncertainty': 2,\n          'darker': 1,\n          'merely': 22,\n          'revival': 1,\n          'atonement': 3,\n          'SOMETIMES': 1,\n          'begging': 4,\n          'So': 27,\n          'about': 135,\n          'conscious': 9,\n          'forfeit': 2,\n          'inform': 5,\n          'HER': 29,\n          'materially': 7,\n          'younger': 6,\n          'merest': 2,\n          'ignorance': 12,\n          'sacrificing': 1,\n          'restrained': 2,\n          'amount': 4,\n          'assisting': 1,\n          'Fanny': 48,\n          'Imagine': 1,\n          'explanations': 2,\n          'discarded': 1,\n          'remarkably': 6,\n          'strikingly': 2,\n          'manly': 3,\n          'next': 52,\n          'forcing': 4,\n          'formally': 1,\n          'natured': 12,\n          'bulk': 1,\n          'benefactress': 1,\n          'change': 29,\n          'unreasonable': 2,\n          'dooming': 1,\n          'leisure': 13,\n          'surprised': 25,\n          'unaccountable': 6,\n          'resisted': 3,\n          'ends': 2,\n          'their': 463,\n          'errors': 1,\n          '!': 289,\n          'assisted': 6,\n          'Elinor': 684,\n          'credulous': 1,\n          'connect': 1,\n          'checking': 3,\n          'am': 270,\n          'frivolous': 1,\n          'militate': 1,\n          'destiny': 1,\n          'observable': 1,\n          'suspended': 3,\n          'shuddering': 1,\n          'last': 101,\n          'accommodate': 3,\n          'consistent': 2,\n          'married': 54,\n          'affectedly': 2,\n          'horrid': 4,\n          'breach': 5,\n          'intended': 13,\n          'placed': 12,\n          'upon': 88,\n          'repine': 1,\n          'acquainted': 26,\n          'disagreements': 1,\n          'draw': 8,\n          'turned': 27,\n          'foretold': 2,\n          'bad': 11,\n          'utter': 2,\n          '--,': 1,\n          'volume': 1,\n          'nephew': 3,\n          'concerns': 8,\n          'somewhat': 3,\n          'reaching': 1,\n          'testimony': 4,\n          'land': 5,\n          'depended': 9,\n          'unhappiness': 7,\n          'This': 59,\n          'Certainly': 12,\n          'overheard': 1,\n          'use': 23,\n          'introduction': 3,\n          'hearted': 10,\n          'reasonably': 10,\n          'set': 33,\n          'prosecution': 1,\n          'estate': 19,\n          'self': 29,\n          'relics': 1,\n          'catching': 8,\n          'differed': 1,\n          'temporizing': 1,\n          'inspiring': 1,\n          'dwelling': 6,\n          'works': 4,\n          'meritorious': 1,\n          ';\"': 5,\n          'reluctantly': 1,\n          'woody': 1,\n          'mother': 258,\n          'bid': 4,\n          'because': 60,\n          'pin': 2,\n          'illness': 12,\n          'life': 58,\n          'admire': 10,\n          'ridiculous': 7,\n          'soul': 20,\n          'depravity': 1,\n          'deficiency': 4,\n          'hung': 4,\n          'energy': 4,\n          'Somehow': 1,\n          'pride': 8,\n          'arise': 7,\n          'divided': 6,\n          'family': 83,\n          'shoulders': 2,\n          'tranquility': 1,\n          'shut': 7,\n          'mortify': 2,\n          'Whatever': 8,\n          'meditated': 2,\n          'species': 2,\n          'promotion': 1,\n          'wishing': 14,\n          'deepest': 1,\n          'reseated': 1,\n          'meant': 21,\n          'guard': 2,\n          'smiles': 2,\n          'expected': 39,\n          'fingers': 3,\n          'afraid': 23,\n          'above': 20,\n          'recurred': 2,\n          'confidante': 1,\n          'worked': 1,\n          'going': 58,\n          'respects': 3,\n          'foundations': 1,\n          'utility': 1,\n          'silk': 1,\n          'bigger': 1,\n          'lower': 3,\n          'honourably': 1,\n          'weary': 4,\n          'possession': 11,\n          'wildest': 1,\n          'few': 77,\n          'negligence': 2,\n          'continuing': 7,\n          'justice': 14,\n          'here': 49,\n          'pleasantness': 1,\n          'cherishing': 1,\n          'accustom': 1,\n          'rising': 7,\n          'warmed': 1,\n          'inch': 1,\n          'approve': 8,\n          'amazing': 1,\n          'square': 2,\n          'doubting': 2,\n          'Reserved': 2,\n          'superiority': 2,\n          'circumspection': 1,\n          'him': 633,\n          'dupe': 1,\n          'idleness': 4,\n          'Supported': 1,\n          'shortly': 7,\n          'Courtland': 2,\n          'conspicuous': 1,\n          'criminal': 1,\n          'perceiving': 6,\n          'resolve': 2,\n          'friend': 55,\n          ',)': 9,\n          'His': 72,\n          'refrain': 1,\n          'returned': 52,\n          'lamentation': 1,\n          'each': 57,\n          'stirring': 3,\n          'persevered': 1,\n          'instigated': 1,\n          'throughout': 1,\n          'striving': 1,\n          'strains': 1,\n          'Like': 3,\n          'Let': 3,\n          'delivered': 5,\n          'relent': 1,\n          'neat': 2,\n          'measure': 8,\n          'send': 11,\n          'wounding': 3,\n          'entertained': 2,\n          'Hush': 1,\n          'horses': 13,\n          'undesirable': 1,\n          'cards': 8,\n          'exclaim': 1,\n          'Grandeur': 1,\n          'unsettle': 1,\n          'appointed': 1,\n          'furnished': 2,\n          'pronouncing': 3,\n          'makes': 6,\n          'extremely': 13,\n          'advantage': 25,\n          'hilarity': 1,\n          'inconvenient': 1,\n          'enjoyed': 4,\n          'conspired': 1,\n          'mine': 17,\n          'embitter': 1,\n          'hauteur': 1,\n          'determine': 9,\n          'bequest': 1,\n          'Somerset': 1,\n          'handsome': 27,\n          'rescued': 1,\n          'HIM': 9,\n          'agitation': 17,\n          'unaccountably': 1,\n          'WITHOUT': 1,\n          'lasted': 7,\n          'hereafter': 6,\n          'construction': 1,\n          'conquest': 4,\n          'detection': 1,\n          'towards': 62,\n          'enfranchisement': 1,\n          'arriving': 2,\n          'poultry': 2,\n          'accepted': 7,\n          'indolent': 1,\n          'expressly': 2,\n          'awaken': 1,\n          'February': 6,\n          'headache': 1,\n          'foreplanned': 1,\n          'concurrence': 1,\n          'residing': 1,\n          'none': 18,\n          'fury': 1,\n          'further': 6,\n          'indulge': 3,\n          'practiced': 1,\n          'turn': 16,\n          'From': 14,\n          'fine': 25,\n          'gardener': 1,\n          'soothe': 4,\n          'opening': 5,\n          'source': 7,\n          'sportsmen': 3,\n          'legacies': 1,\n          'nipped': 1,\n          'shrugging': 1,\n          'turning': 15,\n          'Park': 22,\n          'happiest': 6,\n          'regretted': 3,\n          'variety': 11,\n          'hints': 3,\n          'subdue': 2,\n          'wind': 6,\n          'neglect': 4,\n          'sun': 3,\n          'effectually': 1,\n          'mingle': 1,\n          'unfair': 1,\n          'neighbouring': 1,\n          'penny': 2,\n          'surveying': 1,\n          'controlled': 1,\n          'traced': 1,\n          'among': 19,\n          'em': 1,\n          'don': 16,\n          'forgetting': 1,\n          'considering': 6,\n          'talk': 40,\n          'strangers': 6,\n          'undivided': 1,\n          'officious': 1,\n          'insist': 1,\n          'specimen': 2,\n          'remorse': 1,\n          'liberty': 19,\n          'nodding': 1,\n          'stuff': 2,\n          'letters': 20,\n          'comments': 1,\n          'inflict': 1,\n          'relation': 14,\n          'headed': 1,\n          'urging': 2,\n          'guilt': 7,\n          'passage': 6,\n          'object': 37,\n          'exercised': 1,\n          'cast': 5,\n          'workmen': 1,\n          'avarice': 2,\n          'undertaking': 1,\n          'prudence': 8,\n          'forcibly': 1,\n          'referred': 2,\n          'lap': 2,\n          'vast': 7,\n          'later': 6,\n          'signify': 7,\n          'widen': 1,\n          'unnoticed': 1,\n          'joking': 1,\n          'insignificance': 1,\n          'allowed': 17,\n          'SHOULD': 4,\n          'dashing': 2,\n          'maintenance': 2,\n          'counter': 1,\n          'filigree': 3,\n          'carried': 16,\n          'Whoever': 1,\n          'attempting': 9,\n          'group': 1,\n          'mansion': 4,\n          'whither': 5,\n          '19': 1,\n          'appearing': 6,\n          'much': 287,\n          'Scotland': 1,\n          'handsomest': 2,\n          'condemned': 2,\n          'Me': 2,\n          'sin': 1,\n          'CHAPTER': 50,\n          'Something': 4,\n          'spirited': 5,\n          'independence': 7,\n          'eloquence': 5,\n          'sterling': 1,\n          'writer': 3,\n          'mediation': 2,\n          'esteem': 16,\n          'Watched': 1,\n          'By': 13,\n          'incivility': 2,\n          'deserving': 2,\n          'obedient': 1,\n          'shrubberies': 2,\n          'Disappointment': 1,\n          'rated': 2,\n          'accompanied': 2,\n          'sentence': 8,\n          'sixpence': 1,\n          'figure': 8,\n          'interspersed': 1,\n          'accidental': 1,\n          'moved': 9,\n          'diminution': 2,\n          'la': 3,\n          'Pray': 6,\n          'droll': 7,\n          'encourage': 4,\n          'rings': 1,\n          'nt': 1,\n          'imbibed': 1,\n          'I': 2004,\n          'worried': 2,\n          'departure': 7,\n          'prettiest': 1,\n          'can': 206,\n          'separation': 9,\n          'Farm': 1,\n          'attendant': 1,\n          'festival': 1,\n          'WOULD': 5,\n          'decline': 1,\n          'irrepressible': 2,\n          'falling': 4,\n          'conquests': 2,\n          'valley': 10,\n          'having': 57,\n          'Mamma': 10,\n          'absurd': 2,\n          'wherever': 3,\n          'nicest': 3,\n          'church': 7,\n          'anew': 2,\n          'Three': 1,\n          'submit': 10,\n          'hardily': 1,\n          'impartial': 2,\n          'parts': 5,\n          'fervent': 3,\n          'suspects': 1,\n          'stockings': 2,\n          'out': 161,\n          'enraged': 1,\n          'farm': 3,\n          'vivacity': 2,\n          'unaffectedly': 1,\n          'sellers': 1,\n          'worse': 17,\n          'little': 158,\n          'require': 3,\n          'foreseen': 2,\n          'cease': 5,\n          'windows': 3,\n          'altered': 7,\n          'marries': 1,\n          'conveying': 2,\n          'successful': 1,\n          'Dorsetshire': 6,\n          'provident': 1,\n          'beds': 1,\n          'consulted': 2,\n          'incurring': 2,\n          'inclosing': 1,\n          'demands': 5,\n          'shelter': 2,\n          'Cleveland': 27,\n          'calling': 13,\n          'Surprised': 1,\n          'creatures': 3,\n          'conform': 1,\n          'collecting': 1,\n          'undressed': 1,\n          'dwelt': 2,\n          'courted': 2,\n          'echoed': 1,\n          'dessert': 1,\n          'chanced': 3,\n          'ran': 9,\n          'mixing': 2,\n          'billiards': 1,\n          'wet': 3,\n          'tremour': 1,\n          'fat': 1,\n          'alleged': 1,\n          'pillow': 1,\n          'thin': 2,\n          'William': 3,\n          'meet': 22,\n          'evening': 51,\n          'HIS': 8,\n          'Come': 7,\n          'distressed': 6,\n          'assurance': 14,\n          'treasured': 1,\n          'DRAW': 1,\n          'end': 54,\n          'outside': 1,\n          'detestably': 1,\n          'passionate': 1,\n          'fifteen': 4,\n          'Dear': 7,\n          'enjoyment': 20,\n          'lived': 20,\n          'enquire': 6,\n          'describing': 1,\n          'thoughtless': 1,\n          'Sackville': 1,\n          'infinitely': 4,\n          'gloomy': 3,\n          'sent': 21,\n          'might': 215,\n          'precaution': 2,\n          'indifferent': 16,\n          'begun': 7,\n          'eventually': 2,\n          'consoled': 3,\n          'engagements': 14,\n          'maintained': 6,\n          'parties': 16,\n          'dropped': 6,\n          'sold': 5,\n          'rare': 1,\n          'compound': 2,\n          'son': 35,\n          'Careys': 4,\n          'hunt': 1,\n          'respectful': 2,\n          'lurking': 1,\n          'provoke': 4,\n          'guessing': 1,\n          'died': 8,\n          'white': 5,\n          'demeanor': 1,\n          'willing': 6,\n          'uncommonly': 6,\n          'miles': 10,\n          'presently': 8,\n          'exclamation': 4,\n          'Mr': 178,\n          'exquisite': 4,\n          'deplorable': 1,\n          'submitted': 6,\n          'Had': 25,\n          '9': 1,\n          'suffering': 15,\n          'abhorrence': 4,\n          'presents': 1,\n          'humanity': 3,\n          'exactly': 27,\n          'kicked': 1,\n          'treachery': 1,\n          'art': 2,\n          'dissembling': 1,\n          'observe': 9,\n          'the': 3861,\n          'billiard': 1,\n          'debt': 2,\n          'disappeared': 2,\n          'Edward': 262,\n          'unreasonably': 1,\n          'animating': 2,\n          'constitution': 3,\n          'pained': 6,\n          'unamiable': 1,\n          \".'\": 8,\n          'required': 19,\n          'forbid': 1,\n          'have': 807,\n          'flushed': 1,\n          'gown': 2,\n          'minute': 12,\n          'gift': 4,\n          'exuberance': 1,\n          'visit': 62,\n          'eighty': 1,\n          'spend': 11,\n          'bones': 1,\n          'consolation': 14,\n          'dissuade': 3,\n          'if': 249,\n          'heightened': 3,\n          'names': 2,\n          'previously': 6,\n          'congratulated': 1,\n          'house': 159,\n          'does': 50,\n          'increasing': 5,\n          'involuntary': 3,\n          'conversations': 1,\n          'conforming': 1,\n          'will': 354,\n          'from': 527,\n          '-?\"': 1,\n          'lodgings': 3,\n          'forming': 4,\n          'awful': 3,\n          'farewell': 7,\n          'favour': 27,\n          'lane': 2,\n          'modest': 2,\n          'renewal': 1,\n          'Comparisons': 1,\n          'hearts': 1,\n          'surrounded': 2,\n          'tolerably': 7,\n          'is': 728,\n          'amiable': 17,\n          'torrent': 1,\n          'tenderly': 4,\n          'charmed': 2,\n          'acceptance': 3,\n          'penetrate': 1,\n          'ma': 32,\n          'expression': 13,\n          'reproachfully': 1,\n          'inconsolable': 1,\n          'frightens': 1,\n          'overcome': 18,\n          'solitude': 8,\n          'Kensington': 2,\n          'ourselves': 2,\n          'changing': 1,\n          'dispersed': 1,\n          'fetches': 1,\n          'look': 64,\n          'force': 11,\n          'bye': 5,\n          'listen': 6,\n          'paragraph': 2,\n          'attempted': 11,\n          'prodigious': 4,\n          'remains': 4,\n          'encroachments': 1,\n          'defer': 4,\n          'dining': 10,\n          'sources': 2,\n          'unchanged': 2,\n          'effecting': 1,\n          'day': 150,\n          'beautifully': 1,\n          'veal': 1,\n          'lain': 1,\n          'withhold': 2,\n          ',-': 1,\n          'private': 13,\n          'meetings': 2,\n          'hills': 12,\n          'follows': 2,\n          'suspicions': 5,\n          'high': 10,\n          'discharge': 3,\n          'contemptuously': 1,\n          'seduction': 1,\n          'reproached': 2,\n          'lengthened': 3,\n          'afflictions': 1,\n          'induce': 1,\n          'beat': 1,\n          'flatteries': 2,\n          'Scott': 2,\n          'severe': 6,\n          'NOT': 15,\n          'burnt': 1,\n          'profited': 1,\n          'altar': 2,\n          'assiduous': 1,\n          'overcame': 5,\n          'shop': 7,\n          'naturally': 13,\n          'abuses': 1,\n          'Tis': 1,\n          'turns': 4,\n          'curiosity': 24,\n          'collation': 1,\n          'lamenting': 1,\n          'toleration': 1,\n          'hence': 2,\n          'disgraced': 1,\n          'deemed': 1,\n          'literature': 1,\n          'teach': 2,\n          'widower': 1,\n          'WHERE': 1,\n          'repeatedly': 3,\n          'spared': 11,\n          'listener': 1,\n          'secret': 19,\n          'Sense': 1,\n          'work': 15,\n          'article': 2,\n          'largest': 1,\n          'contents': 4,\n          'burst': 13,\n          'show': 7,\n          'smallness': 3,\n          'disgrace': 5,\n          'dirty': 4,\n          'spends': 1,\n          'steadfast': 1,\n          'worship': 1,\n          'dispatches': 1,\n          'cleared': 3,\n          'reading': 6,\n          'drain': 1,\n          'rushing': 1,\n          'recommending': 1,\n          'highly': 15,\n          'school': 6,\n          'political': 1,\n          'exhilaration': 1,\n          'disgraceful': 2,\n          'sugar': 1,\n          'education': 10,\n          'foibles': 1,\n          'honour': 18,\n          'distrust': 7,\n          'east': 1,\n          'consequent': 3,\n          'driven': 4,\n          'overstrained': 1,\n          'Casino': 1,\n          'distractedly': 1,\n          'secretly': 3,\n          'relieve': 3,\n          'circuit': 1,\n          'policy': 1,\n          'conveniently': 1,\n          'somebody': 3,\n          'invalid': 1,\n          'hair': 18,\n          'hindrance': 2,\n          'Court': 1,\n          'Duty': 1,\n          'accent': 2,\n          'dawned': 1,\n          'kiss': 2,\n          'God': 10,\n          'warmly': 12,\n          'backwardness': 2,\n          'we': 172,\n          'reward': 8,\n          'Brown': 1,\n          'enquired': 1,\n          'correspondence': 4,\n          'sigh': 11,\n          'elegant': 8,\n          'learning': 2,\n          'cold': 23,\n          'debating': 3,\n          'zealously': 1,\n          'carriage': 41,\n          'abruptness': 1,\n          'slightest': 3,\n          'street': 4,\n          'unequal': 1,\n          'employments': 7,\n          'seasons': 1,\n          'seclude': 2,\n          'touched': 1,\n          'stammered': 1,\n          'blushing': 1,\n          'active': 9,\n          'prove': 7,\n          'applied': 3,\n          'cautious': 2,\n          'shewn': 7,\n          'equality': 1,\n          'sends': 2,\n          'Could': 7,\n          'amongst': 6,\n          'enquiries': 5,\n          'philosophy': 1,\n          'pages': 1,\n          'wittiest': 1,\n          'rent': 3,\n          'hates': 1,\n          'Friday': 2,\n          'inforced': 2,\n          'thank': 5,\n          'misinformed': 1,\n          'earth': 5,\n          'examined': 2,\n          'principle': 5,\n          'crept': 1,\n          'demur': 1,\n          ';--': 145,\n          'place': 87,\n          'reserve': 9,\n          'Gardens': 3,\n          'visitor': 8,\n          'inconstant': 1,\n          'thrown': 10,\n          'fortunes': 3,\n          'countenance': 28,\n          'convinced': 31,\n          ',--': 63,\n          'consequently': 2,\n          'MUST': 3,\n          'originated': 3,\n          'contend': 2,\n          'Tell': 8,\n          'helping': 2,\n          'heads': 1,\n          'grave': 16,\n          'hearty': 1,\n          ...})"
                    }, 
                    "execution_count": 10, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "fdist=FreqDist(text2)\nfdist"
        }, 
        {
            "source": "##### There is a method called most_common() that can be conveniently used to show the most frequent words in a frequency distribution.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(',', 9397),\n ('to', 4063),\n ('.', 3975),\n ('the', 3861),\n ('of', 3565),\n ('and', 3350),\n ('her', 2436),\n ('a', 2043),\n ('I', 2004),\n ('in', 1904)]"
                    }, 
                    "execution_count": 17, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "fdist.most_common(10)"
        }, 
        {
            "source": "#### Looking at the most frequent words, you realize that they are not so meaningful. Many words are so commonly used everywhere that they do not reveal anything about the particular document or document collection we are looking at. There are a number of ways to address this problem.\n\n#### We will create a new list text2_long_words and add only words with atleast 5 characters.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "['Sense',\n 'Sensibility',\n 'Austen',\n 'CHAPTER',\n 'family',\n 'Dashwood',\n 'settled',\n 'Sussex',\n 'Their',\n 'estate']"
                    }, 
                    "execution_count": 11, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "text2_long_words=[w for w in text2 if len(w)>=5]\ntext2_long_words[:10]"
        }, 
        {
            "source": "#### Checking the frequency distribution and most common words again on the new list gives more sensible results and shows the major characters in a book.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "fdist2=FreqDist(text2_long_words)\nm=fdist2.most_common(10)"
        }, 
        {
            "source": "## Import Brunel library for visualization", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 22, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "import brunel"
        }, 
        {
            "source": "### Create a dataframe to visualize the common words as a tag cloud using Brunel package.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 54, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "       word   freq\n0    Elinor  684.0\n1     which  592.0\n2     could  568.0\n3  Marianne  566.0\n4     would  507.0\n5     their  463.0\n6     every  361.0\n7    sister  282.0\n8    Edward  262.0\n9    mother  258.0\n"
                }
            ], 
            "cell_type": "code", 
            "source": "import pandas as pd\ndf = pd.DataFrame(columns=['word', 'freq'])\nfor i in m:\n    df.loc[len(df)] = i\n    \nprint (df)\n        "
        }, 
        {
            "source": "## Tag cloud of most common words", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 61, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>", 
                        "text/html": "<!--\n  ~ Copyright (c) 2015 IBM Corporation and others.\n  ~\n  ~ Licensed under the Apache License, Version 2.0 (the \"License\");\n  ~ You may not use this file except in compliance with the License.\n  ~ You may obtain a copy of the License at\n  ~\n  ~     http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing, software\n  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  ~ See the License for the specific language governing permissions and\n  ~ limitations under the License.\n  -->\n\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/c2196ead-44b2-40aa-afbc-20c2d955c746/nbextensions/brunel_ext/brunel.2.3.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/c2196ead-44b2-40aa-afbc-20c2d955c746/nbextensions/brunel_ext/sumoselect.css\">\n\n<style>\n    #visidcb504530-db13-11e7-8f7f-002590fb6998.brunel .chart1 .element1 .element {\n\tfont-family: Impact;\n\tfont-size: 200px;\n}\n</style>\n\n<div id=\"controlsidcb50479c-db13-11e7-8f7f-002590fb6998\" class=\"brunel\"/>\n<svg id=\"visidcb504530-db13-11e7-8f7f-002590fb6998\" width=\"600\" height=\"600\"></svg>"
                    }, 
                    "output_type": "display_data", 
                    "metadata": {}
                }, 
                {
                    "data": {
                        "application/javascript": "/*\n * Copyright (c) 2015 IBM Corporation and others.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * You may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nrequire.config({\n    waitSeconds: 60,\n    paths: {\n        'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n        'topojson': '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n        'brunel' : '/data/jupyter2/c2196ead-44b2-40aa-afbc-20c2d955c746/nbextensions/brunel_ext/brunel.2.3.min',\n        'brunelControls' : '/data/jupyter2/c2196ead-44b2-40aa-afbc-20c2d955c746/nbextensions/brunel_ext/brunel.controls.2.3.min'\n    },\n    shim: {\n       'brunel' : {\n            exports: 'BrunelD3',\n            deps: ['d3', 'topojson'],\n            init: function() {\n               return {\n                 BrunelD3 : BrunelD3,\n                 BrunelData : BrunelData\n              }\n            }\n        },\n       'brunelControls' : {\n            exports: 'BrunelEventHandlers',\n            init: function() {\n               return {\n                 BrunelEventHandlers: BrunelEventHandlers,\n                 BrunelJQueryControlFactory: BrunelJQueryControlFactory\n              }\n            }\n        }\n\n    }\n\n});\n\nrequire([\"d3\"], function(d3) {\n    require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n        function  BrunelVis(visId) {\n  \"use strict\";                                                                       // strict mode\n  var datasets = [],                                      // array of datasets for the original data\n      pre = function(d, i) { return d },                         // default pre-process does nothing\n      post = function(d, i) { return d },                       // default post-process does nothing\n      transitionTime = 200,                                        // transition time for animations\n      charts = [],                                                       // the charts in the system\n      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n\n  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 0, 0, 0, 0),\n      elements = [];                                              // array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .style('cursor', 'default')\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior zoomNone')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_visidcb504530-db13-11e7-8f7f-002590fb6998_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    vis.append('clipPath').attr('id', 'clip_visidcb504530-db13-11e7-8f7f-002590fb6998_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n    var scale_x = d3.scaleLinear(), scale_y = d3.scaleLinear();\n    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n    zoom.on('zoom', function(t, time) {\n        t = t || d3.event.transform;\n        zoomNode.__zoom = t;\n        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n        interior.attr('transform', d3.zoomTransform(zoomNode));\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,                           // data sets passed in and then transformed\n        element, data,                                 // brunel element information and brunel data\n        selection, merged;                                      // d3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1')\n        .attr('transform','translate(' + geom.inner_width/2 + ',' + geom.inner_height/2 + ')'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0)\n          .sort('freq');\n        processed = post(processed, 0);\n        var f0 = processed.field('freq'),\n          f1 = processed.field('word'),\n          f2 = processed.field('#row'),\n          f3 = processed.field('#selection');\n        var keyFunc = function(d) { return f2.value(d) };\n        data = {\n          freq:         function(d) { return f0.value(d.row) },\n          word:         function(d) { return f1.value(d.row) },\n          $row:         function(d) { return f2.value(d.row) },\n          $selection:   function(d) { return f3.value(d.row) },\n          freq_f:       function(d) { return f0.valueFormatted(d.row) },\n          word_f:       function(d) { return f1.valueFormatted(d.row) },\n          $row_f:       function(d) { return f2.valueFormatted(d.row) },\n          $selection_f: function(d) { return f3.valueFormatted(d.row) },\n          _split:       function(d) { return f0.value(d.row)+ '|' + f0.value(d.row) },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n      // Aesthetic Functions\n      var scale_color = d3.scaleLinear().domain([258, 311.25, 364.5, 417.75, 471, 524.25, 577.5, 630.75, 684])\n        .interpolate(d3.interpolateHcl)\n        .range([ '#045a8d', '#2b8cbe', '#74a9cf', '#bdc9e1', '#f8efe8', '#fef0d9', \n          '#fdcc8a', '#fc8d59', '#e34a33']);\n      var color = function(d) { return scale_color(data.freq(d)) };\n      var scale_size = d3.scaleSqrt().domain([0, 684.00007])\n        .range([ 0.001, 1]);\n      var size = function(d) { return scale_size(data.freq(d)) };\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        // Build the cloud layout\n        var cloud = BrunelD3.cloudLayout(processed, [geom.inner_width, geom.inner_height], zoomNode);\n        function keyFunction(d) { return d.key };\n        main.attr('class', 'diagram cloud');\n\n        // Define selection entry operations\n        function initialState(selection) {\n          selection\n            .attr('class', 'element text filled')\n            .style('text-anchor', 'middle').classed('label', true)\n            .text(function(d) { return data.word_f(d) })\n            .style('font-size', function(d) { return (100*size(d)) + '%' })\n            .style('pointer-events', 'none')\n        }\n\n        // Define selection update operations on merged data\n        function updateState(selection) {\n          selection\n            .each(cloud.prepare).call(cloud.build)\n            .filter(BrunelD3.hasData)                     // following only performed for data items\n            .style('fill', color);\n        }\n        // Create selections, set the initial state and transition updates\n        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key });\n        var added = selection.enter().append('text');\n        merged = selection.merge(added);\n        initialState(added);\n        selection.filter(BrunelD3.hasData)\n          .classed('selected', BrunelD3.isSelected(data))\n          .filter(BrunelD3.isSelected(data)).raise();\n        updateState(BrunelD3.transition(merged, transitionMillis));\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); BrunelD3.removeLabels(this); \n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          key:          ['#row'],\n          color:        ['freq'],\n          size:         ['freq']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0;                                           // no transition for first call\n      if ((first || time > -1) && !noData) {\n        elements[0].makeData();\n      }\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   summarized: false,\n   names: ['freq', 'word'], \n   options: ['numeric', 'string'], \n   rows: [[684, 'Elinor'], [592, 'which'], [568, 'could'], [566, 'Marianne'], [507, 'would'],\n  [463, 'their'], [361, 'every'], [282, 'sister'], [262, 'Edward'], [258, 'mother']]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v  = new BrunelVis('visidcb504530-db13-11e7-8f7f-002590fb6998');\nv.build(table1);\n\n    });\n});", 
                        "text/plain": "<IPython.core.display.Javascript object>"
                    }, 
                    "execution_count": 61, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "%%brunel cloud color(freq) size(freq) sort(freq)\nlabel(word) style('font-size:200px;font-family:Impact') legends(none) :: width = 600, height=600"
        }, 
        {
            "source": "## StopWord Removal\nNLTK also has a built-in stop word list for English that can come in handy when we need to remove stop\nwords from a text collection.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "['Sense',\n 'Sensibility',\n 'Austen',\n 'CHAPTER',\n 'family',\n 'Dashwood',\n 'settled',\n 'Sussex',\n 'Their',\n 'estate',\n 'large',\n 'residence',\n 'Norland',\n 'centre',\n 'property',\n 'generations',\n 'lived',\n 'respectable',\n 'manner',\n 'engage',\n 'general',\n 'opinion',\n 'surrounding',\n 'acquaintance',\n 'owner',\n 'estate',\n 'single',\n 'lived',\n 'advanced',\n 'years',\n 'constant',\n 'companion',\n 'housekeeper',\n 'sister',\n 'death',\n 'happened',\n 'years',\n 'produced',\n 'great',\n 'alteration',\n 'supply',\n 'invited',\n 'received',\n 'house',\n 'family',\n 'nephew',\n 'Henry',\n 'Dashwood',\n 'legal',\n 'inheritor',\n 'Norland',\n 'estate',\n 'person',\n 'intended',\n 'bequeath',\n 'society',\n 'nephew',\n 'niece',\n 'children',\n 'Gentleman',\n 'comfortably',\n 'spent',\n 'attachment',\n 'increased',\n 'constant',\n 'attention',\n 'Henry',\n 'Dashwood',\n 'wishes',\n 'proceeded',\n 'merely',\n 'interest',\n 'goodness',\n 'heart',\n 'every',\n 'degree',\n 'solid',\n 'comfort',\n 'could',\n 'receive',\n 'cheerfulness',\n 'children',\n 'added',\n 'relish',\n 'existence',\n 'former',\n 'marriage',\n 'Henry',\n 'Dashwood',\n 'present',\n 'three',\n 'daughters',\n 'steady',\n 'respectable',\n 'young',\n 'amply',\n 'provided',\n 'fortune',\n 'mother',\n 'large',\n 'devolved',\n 'coming',\n 'marriage',\n 'likewise',\n 'happened',\n 'afterwards',\n 'added',\n 'wealth',\n 'therefore',\n 'succession',\n 'Norland',\n 'estate',\n 'really',\n 'important',\n 'sisters',\n 'fortune',\n 'independent',\n 'might',\n 'arise',\n 'father',\n 'inheriting',\n 'property',\n 'could',\n 'small',\n 'Their',\n 'mother',\n 'nothing',\n 'father',\n 'seven',\n 'thousand',\n 'pounds',\n 'disposal',\n 'remaining',\n 'moiety',\n 'first',\n 'fortune',\n 'secured',\n 'child',\n 'interest',\n 'gentleman',\n 'almost',\n 'every',\n 'disappointment',\n 'pleasure',\n 'neither',\n 'unjust',\n 'ungrateful',\n 'leave',\n 'estate',\n 'nephew',\n 'terms',\n 'destroyed',\n 'value',\n 'bequest',\n 'Dashwood',\n 'wished',\n 'daughters',\n 'child',\n 'years',\n 'secured',\n 'leave',\n 'power',\n 'providing',\n 'needed',\n 'provision',\n 'charge',\n 'estate',\n 'valuable',\n 'woods',\n 'whole',\n 'benefit',\n 'child',\n 'occasional',\n 'visits',\n 'father',\n 'mother',\n 'Norland',\n 'gained',\n 'affections',\n 'uncle',\n 'attractions',\n 'means',\n 'unusual',\n 'children',\n 'three',\n 'years',\n 'imperfect',\n 'articulation',\n 'earnest',\n 'desire',\n 'cunning',\n 'tricks',\n 'great',\n 'noise',\n 'outweigh',\n 'value',\n 'attention',\n 'years',\n 'received',\n 'niece',\n 'daughters',\n 'meant',\n 'unkind',\n 'however',\n 'affection',\n 'three',\n 'girls',\n 'thousand',\n 'pounds',\n 'piece',\n 'Dashwood',\n 'disappointment',\n 'first',\n 'severe',\n 'temper',\n 'cheerful',\n 'sanguine',\n 'might',\n 'reasonably',\n 'years',\n 'living',\n 'economically',\n 'considerable',\n 'produce',\n 'estate',\n 'already',\n 'large',\n 'capable',\n 'almost',\n 'immediate',\n 'improvement',\n 'fortune',\n 'tardy',\n 'coming',\n 'twelvemonth',\n 'survived',\n 'uncle',\n 'longer',\n 'thousand',\n 'pounds',\n 'including',\n 'legacies',\n 'remained',\n 'widow',\n 'daughters',\n 'danger',\n 'known',\n 'Dashwood',\n 'recommended',\n 'strength',\n 'urgency',\n 'illness',\n 'could',\n 'command',\n 'interest',\n 'mother',\n 'sisters',\n 'Dashwood',\n 'strong',\n 'feelings',\n 'family',\n 'affected',\n 'recommendation',\n 'nature',\n 'promised',\n 'every',\n 'thing',\n 'power',\n 'comfortable',\n 'father',\n 'rendered',\n 'assurance',\n 'Dashwood',\n 'leisure',\n 'consider',\n 'might',\n 'prudently',\n 'power',\n 'disposed',\n 'young',\n 'unless',\n 'rather',\n 'hearted',\n 'rather',\n 'selfish',\n 'disposed',\n 'general',\n 'respected',\n 'conducted',\n 'propriety',\n 'discharge',\n 'ordinary',\n 'duties',\n 'married',\n 'amiable',\n 'woman',\n 'might',\n 'still',\n 'respectable',\n 'might',\n 'amiable',\n 'young',\n 'married',\n 'Dashwood',\n 'strong',\n 'caricature',\n 'narrow',\n 'minded',\n 'selfish',\n 'promise',\n 'father',\n 'meditated',\n 'within',\n 'increase',\n 'fortunes',\n 'sisters',\n 'present',\n 'thousand',\n 'pounds',\n 'piece',\n 'really',\n 'thought',\n 'equal',\n 'prospect',\n 'thousand',\n 'addition',\n 'present',\n 'income',\n 'besides',\n 'remaining',\n 'mother',\n 'fortune',\n 'warmed',\n 'heart',\n 'capable',\n 'generosity',\n 'would',\n 'three',\n 'thousand',\n 'pounds',\n 'would',\n 'liberal',\n 'handsome',\n 'would',\n 'enough',\n 'completely',\n 'Three',\n 'thousand',\n 'pounds',\n 'could',\n 'spare',\n 'considerable',\n 'little',\n 'inconvenience',\n 'thought',\n 'successively',\n 'repent',\n 'sooner',\n 'father',\n 'funeral',\n 'Dashwood',\n 'without',\n 'sending',\n 'notice',\n 'intention',\n 'mother',\n 'arrived',\n 'child',\n 'attendants',\n 'could',\n 'dispute',\n 'right',\n 'house',\n 'husband',\n 'moment',\n 'father',\n 'decease',\n 'indelicacy',\n 'conduct',\n 'greater',\n 'woman',\n 'Dashwood',\n 'situation',\n 'common',\n 'feelings',\n 'highly',\n 'unpleasing',\n 'sense',\n 'honor',\n 'generosity',\n 'romantic',\n 'offence',\n 'whomsoever',\n 'given',\n 'received',\n 'source',\n 'immoveable',\n 'disgust',\n 'Dashwood',\n 'never',\n 'favourite',\n 'husband',\n 'family',\n 'opportunity',\n 'present',\n 'shewing',\n 'little',\n 'attention',\n 'comfort',\n 'people',\n 'could',\n 'occasion',\n 'required',\n 'acutely',\n 'Dashwood',\n 'ungracious',\n 'behaviour',\n 'earnestly',\n 'despise',\n 'daughter',\n 'arrival',\n 'latter',\n 'would',\n 'quitted',\n 'house',\n 'entreaty',\n 'eldest',\n 'induced',\n 'first',\n 'reflect',\n 'propriety',\n 'going',\n 'tender',\n 'three',\n 'children',\n 'determined',\n 'afterwards',\n 'sakes',\n 'avoid',\n 'breach',\n 'brother',\n 'Elinor',\n 'eldest',\n 'daughter',\n 'whose',\n 'advice',\n 'effectual',\n 'possessed',\n 'strength',\n 'understanding',\n 'coolness',\n 'judgment',\n 'qualified',\n 'though',\n 'nineteen',\n 'counsellor',\n 'mother',\n 'enabled',\n 'frequently',\n 'counteract',\n 'advantage',\n 'eagerness',\n 'Dashwood',\n 'generally',\n 'imprudence',\n 'excellent',\n 'heart',\n 'disposition',\n 'affectionate',\n 'feelings',\n 'strong',\n 'govern',\n 'knowledge',\n 'mother',\n 'learn',\n 'sisters',\n 'resolved',\n 'never',\n 'taught',\n 'Marianne',\n 'abilities',\n 'respects',\n 'quite',\n 'equal',\n 'Elinor',\n 'sensible',\n 'clever',\n 'eager',\n 'everything',\n 'sorrows',\n 'could',\n 'moderation',\n 'generous',\n 'amiable',\n 'interesting',\n 'everything',\n 'prudent',\n 'resemblance',\n 'mother',\n 'strikingly',\n 'great',\n 'Elinor',\n 'concern',\n 'excess',\n 'sister',\n 'sensibility',\n 'Dashwood',\n 'valued',\n 'cherished',\n 'encouraged',\n 'violence',\n 'affliction',\n 'agony',\n 'grief',\n 'overpowered',\n 'first',\n 'voluntarily',\n 'renewed',\n 'sought',\n 'created',\n 'wholly',\n 'sorrow',\n 'seeking',\n 'increase',\n 'wretchedness',\n 'every',\n 'reflection',\n 'could',\n 'afford',\n 'resolved',\n 'admitting',\n 'consolation',\n 'future',\n 'Elinor',\n 'deeply',\n 'afflicted',\n 'still',\n 'could',\n 'struggle',\n 'could',\n 'exert',\n 'could',\n 'consult',\n 'brother',\n 'could',\n 'receive',\n 'sister',\n 'arrival',\n 'treat',\n 'proper',\n 'attention',\n 'could',\n 'strive',\n 'rouse',\n 'mother',\n 'similar',\n 'exertion',\n 'encourage',\n 'similar',\n 'forbearance',\n 'Margaret',\n 'sister',\n 'humored',\n 'disposed',\n 'already',\n 'imbibed',\n 'Marianne',\n 'romance',\n 'without',\n 'sense',\n 'thirteen',\n 'equal',\n 'sisters',\n 'advanced',\n 'period',\n 'CHAPTER',\n 'Dashwood',\n 'installed',\n 'mistress',\n 'Norland',\n 'mother',\n 'sisters',\n 'degraded',\n 'condition',\n 'visitors',\n 'however',\n 'treated',\n 'quiet',\n 'civility',\n 'husband',\n 'kindness',\n 'could',\n 'towards',\n 'anybody',\n 'beyond',\n 'child',\n 'really',\n 'pressed',\n 'earnestness',\n 'consider',\n 'Norland',\n 'appeared',\n 'eligible',\n 'Dashwood',\n 'remaining',\n 'could',\n 'accommodate',\n 'house',\n 'neighbourhood',\n 'invitation',\n 'accepted',\n 'continuance',\n 'place',\n 'everything',\n 'reminded',\n 'former',\n 'delight',\n 'exactly',\n 'suited',\n 'seasons',\n 'cheerfulness',\n 'temper',\n 'could',\n 'cheerful',\n 'possess',\n 'greater',\n 'degree',\n 'sanguine',\n 'expectation',\n 'happiness',\n 'happiness',\n 'sorrow',\n 'equally',\n 'carried',\n 'fancy',\n 'beyond',\n 'consolation',\n 'pleasure',\n 'beyond',\n 'alloy',\n 'Dashwood',\n 'approve',\n 'husband',\n 'intended',\n 'sisters',\n 'three',\n 'thousand',\n 'pounds',\n 'fortune',\n 'little',\n 'would',\n 'impoverishing',\n 'dreadful',\n 'degree',\n 'begged',\n 'think',\n 'subject',\n 'could',\n 'answer',\n 'child',\n 'child',\n 'large',\n 'possible',\n 'claim',\n 'could',\n 'Dashwoods',\n 'related',\n 'blood',\n 'considered',\n 'relationship',\n 'generosity',\n 'large',\n 'amount',\n 'known',\n 'affection',\n 'supposed',\n 'exist',\n 'children',\n 'different',\n 'marriages',\n 'little',\n 'Harry',\n 'giving',\n 'money',\n 'sisters',\n 'father',\n 'request',\n 'replied',\n 'husband',\n 'assist',\n 'widow',\n 'daughters',\n 'talking',\n 'light',\n 'headed',\n 'right',\n 'senses',\n 'could',\n 'thought',\n 'thing',\n 'begging',\n 'fortune',\n 'child',\n 'stipulate',\n 'particular',\n 'Fanny',\n 'requested',\n 'general',\n 'terms',\n 'assist',\n 'situation',\n 'comfortable',\n 'power',\n 'Perhaps',\n 'would',\n 'wholly',\n 'could',\n 'hardly',\n 'suppose',\n 'neglect',\n 'required',\n 'promise',\n 'could',\n 'least',\n 'thought',\n 'promise',\n 'therefore',\n 'given',\n 'performed',\n 'Something',\n 'whenever',\n 'leave',\n 'Norland',\n 'settle',\n 'something',\n 'something',\n 'three',\n 'thousand',\n 'pounds',\n 'Consider',\n 'added',\n 'money',\n 'parted',\n 'never',\n 'return',\n 'sisters',\n 'marry',\n 'indeed',\n 'could',\n 'restored',\n 'little',\n 'husband',\n 'gravely',\n 'would',\n 'great',\n 'difference',\n 'Harry',\n 'regret',\n 'large',\n 'parted',\n 'numerous',\n 'family',\n 'instance',\n 'would',\n 'convenient',\n 'addition',\n 'would',\n 'Perhaps',\n 'would',\n 'better',\n 'parties',\n 'diminished',\n 'hundred',\n 'pounds',\n 'would',\n 'prodigious',\n 'increase',\n 'fortunes',\n 'beyond',\n 'anything',\n 'great',\n 'brother',\n 'earth',\n 'would',\n 'sisters',\n 'REALLY',\n 'sisters',\n 'blood',\n 'generous',\n 'spirit',\n 'would',\n 'thing',\n 'replied',\n 'rather',\n 'occasions',\n 'little',\n 'least',\n 'think',\n 'enough',\n 'hardly',\n 'expect',\n 'There',\n 'knowing',\n 'expect',\n 'think',\n 'expectations',\n 'question',\n 'afford',\n 'Certainly',\n 'think',\n 'afford',\n 'hundred',\n 'pounds',\n 'piece',\n 'without',\n 'addition',\n 'three',\n 'thousand',\n 'pounds',\n 'mother',\n 'death',\n 'comfortable',\n 'fortune',\n 'young',\n 'woman',\n 'indeed',\n 'strikes',\n 'addition',\n 'thousand',\n 'pounds',\n 'divided',\n 'amongst',\n 'marry',\n 'comfortably',\n 'together',\n 'interest',\n 'thousand',\n 'pounds',\n 'therefore',\n 'whether',\n 'whole',\n 'would',\n 'advisable',\n 'something',\n 'mother',\n 'lives',\n 'rather',\n 'something',\n 'annuity',\n 'sisters',\n 'would',\n 'effects',\n 'hundred',\n 'would',\n 'perfectly',\n 'comfortable',\n 'hesitated',\n 'little',\n 'however',\n 'giving',\n 'consent',\n 'better',\n 'parting',\n 'fifteen',\n 'hundred',\n 'pounds',\n 'Dashwood',\n 'fifteen',\n 'years',\n 'shall',\n 'completely',\n 'taken',\n 'Fifteen',\n 'years',\n 'Fanny',\n 'cannot',\n 'worth',\n 'purchase',\n 'Certainly',\n 'observe',\n 'people',\n 'always',\n 'annuity',\n 'stout',\n 'healthy',\n 'hardly',\n 'forty',\n 'annuity',\n 'serious',\n 'business',\n 'comes',\n 'every',\n 'getting',\n 'aware',\n 'known',\n 'great',\n 'trouble',\n 'annuities',\n 'mother',\n 'clogged',\n 'payment',\n 'three',\n 'superannuated',\n 'servants',\n 'father',\n 'amazing',\n 'disagreeable',\n 'found',\n 'Twice',\n 'every',\n 'annuities',\n 'trouble',\n 'getting',\n 'afterwards',\n 'turned',\n 'thing',\n 'mother',\n 'quite',\n 'income',\n 'perpetual',\n 'claims',\n 'unkind',\n 'father',\n 'otherwise',\n 'money',\n 'would',\n 'entirely',\n 'mother',\n 'disposal',\n 'without',\n 'restriction',\n 'whatever',\n 'given',\n 'abhorrence',\n 'annuities',\n 'would',\n 'payment',\n 'world',\n 'certainly',\n 'unpleasant',\n 'thing',\n 'replied',\n 'Dashwood',\n 'yearly',\n 'drains',\n 'income',\n 'fortune',\n 'mother',\n 'justly',\n 'regular',\n 'payment',\n 'every',\n 'means',\n 'desirable',\n 'takes',\n 'independence',\n 'Undoubtedly',\n 'thanks',\n 'think',\n 'secure',\n 'expected',\n 'raises',\n 'gratitude',\n 'whatever',\n 'discretion',\n 'entirely',\n 'would',\n 'allow',\n 'thing',\n 'yearly',\n 'inconvenient',\n 'years',\n 'spare',\n 'hundred',\n 'fifty',\n 'pounds',\n 'expenses',\n 'believe',\n 'right',\n 'better',\n 'annuity',\n 'whatever',\n 'occasionally',\n 'greater',\n 'assistance',\n 'yearly',\n 'allowance',\n 'would',\n 'enlarge',\n 'style',\n 'living',\n 'larger',\n 'income',\n 'would',\n 'sixpence',\n 'richer',\n 'certainly',\n 'present',\n 'fifty',\n 'pounds',\n ...]"
                    }, 
                    "execution_count": 13, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "from nltk.corpus import stopwords\nstop_list=stopwords.words('english')\ntext2_stopremoved=[w for w in text2_long_words if w not in stop_list]\ntext2_stopremoved"
        }, 
        {
            "source": "## Stemming\nNLTK also has a built-in Porter stemmer we can use.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 24, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "from nltk.stem.porter import *\nstemmer=PorterStemmer()\n\ntext2_stemmed=[stemmer.stem(w) for w in text2_stopremoved]\n    \n    \n\n#type(text2_stemmed)"
        }, 
        {
            "source": "## Gensim\n\nA Python library that provides some built-in functions for easily converting\ndocuments to vectors and computing cosine similarities.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 91, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Collecting gensim\n  Downloading gensim-3.1.0.tar.gz (15.1MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.1MB 83kB/s  eta 0:00:01\n\u001b[?25hCollecting numpy>=1.11.3 (from gensim)\n  Downloading numpy-1.13.3-cp35-cp35m-manylinux1_x86_64.whl (16.9MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16.9MB 73kB/s  eta 0:00:01\n\u001b[?25hCollecting scipy>=0.18.1 (from gensim)\n  Downloading scipy-1.0.0-cp35-cp35m-manylinux1_x86_64.whl (49.6MB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 49.6MB 24kB/s  eta 0:00:01    71% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589         | 35.4MB 14.8MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): six>=1.5.0 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sfbc-20c2d955c74628-3c618564d05f/.local/lib/python3.5/site-packages (from gensim)\nCollecting smart_open>=1.2.1 (from gensim)\n  Downloading smart_open-1.5.5.tar.gz\nRequirement already satisfied (use --upgrade to upgrade): boto>=2.32 in /usr/local/src/conda3_runtime.v24/4.1.1/lib/python3.5/site-packages (from smart_open>=1.2.1->gensim)\nCollecting bz2file (from smart_open>=1.2.1->gensim)\n  Downloading bz2file-0.98.tar.gz\nRequirement already satisfied (use --upgrade to upgrade): requests in /usr/local/src/conda3_runtime.v24/4.1.1/lib/python3.5/site-packages (from smart_open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): boto3 in /usr/local/src/conda3_runtime.v24/4.1.1/lib/python3.5/site-packages (from smart_open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): botocore<1.6.0,>=1.5.0 in /usr/local/src/conda3_runtime.v24/4.1.1/lib/python3.5/site-packages (from boto3->smart_open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): s3transfer<0.2.0,>=0.1.10 in /usr/local/src/conda3_runtime.v24/4.1.1/lib/python3.5/site-packages (from boto3->smart_open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): jmespath<1.0.0,>=0.7.1 in /usr/local/src/conda3_runtime.v24/4.1.1/lib/python3.5/site-packages (from boto3->smart_open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil<3.0.0,>=2.1 in /usr/local/src/conda3_runtime.v24/4.1.1/lib/python3.5/site-packages (from botocore<1.6.0,>=1.5.0->boto3->smart_open>=1.2.1->gensim)\nRequirement already satisfied (use --upgrade to upgrade): docutils>=0.10 in /usr/local/src/conda3_runtime.v24/4.1.1/lib/python3.5/site-packages (from botocore<1.6.0,>=1.5.0->boto3->smart_open>=1.2.1->gensim)\nBuilding wheels for collected packages: gensim, smart-open, bz2file\n  Running setup.py bdist_wheel for gensim ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/.cache/pip/wheels/a2/81/9e/37cdb84294955eb03e9b9898a0827fc80c4ce1cd19e6be096e\n  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/.cache/pip/wheels/ed/64/56/a922ace26f5d32090849ec8d89192b2b9ff0d280a5be0a4b7b\n  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /gpfs/fs01/user/sfbc-20c2d955c74628-3c618564d05f/.cache/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\nSuccessfully built gensim smart-open bz2file\nInstalling collected packages: numpy, scipy, bz2file, smart-open, gensim\nSuccessfully installed bz2file-0.98 gensim-3.1.0 numpy-1.13.3 scipy-1.0.0 smart-open-1.5.5\n"
                }
            ], 
            "cell_type": "code", 
            "source": "!pip install gensim"
        }, 
        {
            "source": "## Sparse Vectors\nTo convert a piece of text into a vector, we need to first determine the dimension of the vector, or in other\nwords the number of components of the vector. Generally, the dimension of the vectors representing documents\n(and thus the dimension of the vector space) is the same as the vocabulary size, that is, each unique\nword corresponds to an entry of the vectors. Here vocabulary refers to the set of all unique words in a corpus.\nWhile finding out the vocabulary size is not a problem\u2014you may want to think about how to do this\u2014\nthe real problem is that for a real world corpus, the vocabulary is usually very large. It is not uncommon to\nhave millions of words in a vocabulary. If we represent each document as a very high-dimensional vector,\nwe will need a lot of space to store these vectors either in memory or on disk. In reality, however, each\ndocument contains only a relatively small set of words, so the vector used to represent a document has\nmost entries equal to zero and a small subset of entries with non-zero values. When we store such kind of\nvectors in a computer, we typically use a sparse vector representation. \n\nFor example, suppose our vocabulary has 10 words. Let us look at the following vector:\nv =(1,0,0,2,0,0,0,0,0,0.5)\n\nTo store this vector in its original form, we need to store 10 numbers, each corresponding to one entry of the\nvector. A sparse vector representation stores only the non-zero entries as follows:\nv = ((0, 1), (3,2), (9, 0.5)) \nHere the sparse vector is a list of pairs. For each pair, the first number is an ID or index indicating a particular\nentry of the original vector. For example, (0, 1) indicates that the first entry of the original v has a value of\n1, and (9, 0.5) indicates that the tenth entry of the original v has a value of 0.5. We can see that the amount\nof space needed to store this sparse vector is now reduced to twice the number of non-zero entries of the\noriginal vector.\n\n## Creating a Dictionary from a Corpus\n\nNow think about converting all documents in a corpus into vectors. We need to map each unique word in\nthe vocabulary of this corpus to an ID or index first. These mappings from words to IDs are represented by\na class called Dictionary in Gensim.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 25, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'household': 0,\n 'face': 1,\n 'outrun': 2,\n 'anim': 3,\n 'gratif': 4,\n 'vari': 5,\n 'gloom': 2832,\n 'afford': 6,\n 'mad': 7,\n 'unexpectedli': 8,\n 'undesir': 9,\n 'insult': 10,\n 'spoken': 11,\n 'thirteen': 12,\n 'addit': 13,\n 'proof': 14,\n 'loiter': 15,\n 'brief': 16,\n 'pastur': 17,\n 'malic': 3382,\n 'forsak': 1163,\n 'impos': 19,\n 'fall': 20,\n 'exuber': 21,\n 'dawlish': 22,\n 'brandon': 23,\n 'shoe': 24,\n 'mutter': 25,\n 'deject': 26,\n 'resid': 27,\n 'right': 28,\n 'halloo': 29,\n 'proprietor': 30,\n 'somebodi': 31,\n 'apologis': 32,\n 'patron': 34,\n 'regain': 35,\n 'wild': 36,\n 'convinc': 37,\n 'complain': 243,\n 'appeas': 3511,\n 'necess': 2361,\n 'absent': 40,\n 'asund': 3454,\n 'troublesom': 41,\n 'dissatisfi': 42,\n 'alon': 2783,\n 'dread': 43,\n 'chang': 44,\n 'sooth': 45,\n 'bowl': 46,\n 'qualifi': 47,\n 'moder': 48,\n 'mix': 2366,\n 'apricot': 50,\n 'injur': 51,\n 'fear': 52,\n 'quarter': 54,\n 'curat': 55,\n 'compass': 56,\n 'resist': 57,\n 'profit': 58,\n 'palmer': 59,\n 'endeavor': 60,\n 'liber': 61,\n 'refrain': 62,\n 'breakfast': 63,\n 'blockhead': 64,\n 'fetch': 65,\n 'rain': 66,\n 'whether': 67,\n 'metaphor': 68,\n 'unwound': 2957,\n 'confid': 71,\n 'roar': 72,\n 'puls': 73,\n 'spurn': 2372,\n 'parti': 75,\n 'instant': 490,\n 'horseback': 77,\n 'prosper': 78,\n 'easter': 79,\n 'obedi': 80,\n 'behav': 81,\n 'abroad': 82,\n 'unfortun': 83,\n 'wipe': 84,\n 'accident': 85,\n 'rather': 86,\n 'casement': 87,\n 'particularli': 88,\n 'countri': 89,\n 'romanc': 90,\n 'substanc': 92,\n 'former': 93,\n 'benign': 1959,\n 'handsom': 94,\n 'precis': 95,\n 'issu': 938,\n 'deter': 96,\n 'romant': 97,\n 'detect': 98,\n 'unsuit': 99,\n 'chariot': 100,\n 'someth': 101,\n 'devolv': 102,\n 'while': 103,\n 'headach': 2964,\n 'barton': 105,\n 'black': 106,\n 'bestow': 107,\n 'apprehens': 108,\n 'knave': 109,\n 'guilti': 110,\n 'surpris': 111,\n 'proud': 112,\n 'dote': 3355,\n 'thunderbolt': 113,\n 'deriv': 114,\n 'beneath': 115,\n 'incomprehens': 116,\n 'exultingli': 117,\n 'crowd': 118,\n 'sometim': 2734,\n 'feel': 119,\n 'steward': 120,\n 'persev': 121,\n 'benevol': 122,\n 'discont': 123,\n 'oldest': 2744,\n 'conclud': 3522,\n 'swallow': 124,\n 'narrow': 125,\n 'affair': 126,\n 'hussi': 2968,\n 'boil': 128,\n 'servil': 130,\n 'varieti': 131,\n 'cautiou': 132,\n 'misappli': 133,\n 'gentil': 134,\n 'commerci': 135,\n 'let': 136,\n 'misl': 137,\n 'safeguard': 138,\n 'obviat': 139,\n 'brown': 3550,\n 'glimps': 141,\n 'scotland': 142,\n 'extort': 143,\n 'inheritor': 144,\n 'choic': 145,\n 'cri': 146,\n 'pearl': 147,\n 'need': 927,\n 'theft': 151,\n 'feign': 152,\n 'delight': 154,\n 'accept': 155,\n 'sourc': 156,\n 'henceforth': 594,\n 'cold': 158,\n 'older': 160,\n 'suitor': 161,\n 'resent': 999,\n 'cautious': 1000,\n 'conscienti': 1009,\n 'bath': 2975,\n 'hither': 165,\n 'valueless': 166,\n 'strictest': 167,\n 'ivori': 168,\n 'walnut': 1790,\n 'greatli': 170,\n 'honest': 171,\n 'cultiv': 172,\n 'charm': 3148,\n 'littl': 173,\n 'railleri': 174,\n 'tax': 175,\n 'nasti': 176,\n 'accur': 177,\n 'sleepless': 178,\n 'inhabit': 180,\n 'bluntli': 181,\n 'unavail': 182,\n 'disquiet': 2978,\n 'profess': 184,\n 'similar': 185,\n 'firmer': 186,\n 'henceforward': 187,\n 'tower': 188,\n 'determin': 189,\n 'stanhil': 190,\n 'exampl': 191,\n 'unworthi': 192,\n 'entir': 193,\n 'davi': 1797,\n 'straight': 195,\n 'uniform': 196,\n 'courag': 197,\n 'phrase': 198,\n 'simpson': 199,\n 'assuag': 200,\n 'stomach': 201,\n 'poultri': 202,\n 'warmth': 203,\n 'dure': 205,\n 'hanov': 206,\n 'affirm': 207,\n 'conjectur': 208,\n 'potent': 209,\n 'smallest': 210,\n 'penetr': 211,\n 'excus': 212,\n 'articl': 213,\n 'curaci': 214,\n 'earnest': 215,\n 'cheerful': 216,\n 'winter': 217,\n 'acacia': 1801,\n 'about': 219,\n 'conduit': 220,\n 'conscious': 221,\n 'inform': 222,\n 'restrain': 1802,\n 'thistl': 224,\n 'reprov': 225,\n 'appeal': 226,\n 'younger': 227,\n 'libertin': 228,\n 'domest': 229,\n 'merest': 230,\n 'melancholi': 232,\n 'born': 233,\n 'possibl': 234,\n 'diffus': 235,\n 'mental': 236,\n '7000l': 237,\n 'newer': 238,\n 'unpromis': 239,\n 'tribut': 240,\n 'negoti': 242,\n 'insens': 38,\n 'justic': 244,\n 'pack': 245,\n 'incom': 246,\n 'univers': 247,\n 'trivial': 248,\n 'surpass': 249,\n 'benefactress': 250,\n 'holburn': 1520,\n 'seldom': 252,\n 'uncl': 608,\n 'stuf': 254,\n 'recept': 255,\n 'reap': 256,\n 'justif': 257,\n 'mistak': 258,\n 'warmest': 259,\n 'harri': 260,\n 'their': 261,\n 'jargon': 263,\n 'connect': 264,\n 'mingl': 265,\n 'extern': 266,\n 'satisfi': 267,\n 'guid': 268,\n 'recurr': 2400,\n 'last': 270,\n 'situat': 613,\n 'everi': 272,\n 'tile': 273,\n 'attract': 274,\n 'juvenil': 275,\n 'breach': 276,\n 'candl': 277,\n 'report': 278,\n 'sight': 3215,\n 'built': 279,\n 'draw': 280,\n 'risen': 281,\n 'impli': 282,\n 'carey': 283,\n 'middleton': 284,\n 'unemploy': 285,\n 'temper': 286,\n 'utter': 287,\n 'note': 288,\n 'unison': 289,\n 'crimin': 290,\n 'establish': 1791,\n 'journey': 33,\n 'somewhat': 2404,\n 'prepossess': 294,\n 'land': 295,\n 'hostess': 296,\n 'spiritless': 297,\n 'disturb': 298,\n 'oftenest': 3000,\n 'overheard': 300,\n 'stupid': 1819,\n 'use': 302,\n 'pretens': 3001,\n 'death': 1615,\n 'pupil': 305,\n 'repetit': 1821,\n 'repaid': 307,\n 'set': 308,\n 'reviv': 309,\n 'reli': 310,\n 'prior': 311,\n 'anoth': 312,\n 'frugal': 313,\n 'charg': 1948,\n 'counsellor': 315,\n 'fetter': 316,\n 'print': 317,\n 'awok': 318,\n 'butcher': 319,\n 'cloth': 3093,\n 'clearli': 320,\n 'strongli': 321,\n 'amiss': 322,\n 'mention': 2000,\n 'case': 2003,\n 'privaci': 325,\n 'bid': 326,\n 'misconstruct': 327,\n 'bedroom': 328,\n 'partner': 329,\n 'justifi': 330,\n 'porter': 2058,\n 'soul': 332,\n 'robert': 333,\n 'posit': 334,\n 'requisit': 149,\n 'unthought': 337,\n 'impud': 2815,\n 'clamor': 338,\n 'provoc': 339,\n 'sheet': 340,\n 'valley': 341,\n 'calmli': 342,\n 'decid': 343,\n 'villain': 3010,\n 'despatch': 345,\n 'piano': 1221,\n 'templ': 1829,\n 'relinquish': 349,\n 'doctor': 350,\n 'spent': 2415,\n 'perturb': 352,\n 'sympathis': 353,\n 'unintent': 354,\n 'discrimin': 355,\n 'ladyship': 356,\n 'honor': 3119,\n 'betray': 1246,\n 'deepest': 357,\n 'promptitud': 358,\n 'recal': 1832,\n 'meant': 360,\n 'educ': 361,\n 'guard': 362,\n 'nearest': 363,\n 'pretenc': 364,\n 'startl': 365,\n 'unconsci': 3017,\n 'afraid': 367,\n 'colour': 368,\n 'separ': 369,\n 'enamour': 370,\n 'clog': 372,\n 'jumbl': 373,\n 'dissatisfact': 374,\n 'silk': 375,\n 'bigger': 376,\n 'lower': 377,\n 'regular': 705,\n 'concurr': 3356,\n 'eventu': 2420,\n 'wildest': 380,\n 'unsati': 381,\n 'knack': 382,\n 'exult': 3517,\n 'outgrown': 383,\n 'begin': 630,\n 'environ': 385,\n 'lobbi': 386,\n 'applaud': 3020,\n 'amazingli': 388,\n 'temptat': 3458,\n 'commun': 389,\n 'bartlett': 390,\n 'enjoy': 391,\n 'counteract': 392,\n 'gentli': 393,\n 'compos': 394,\n 'moreov': 395,\n 'distinct': 396,\n 'neither': 397,\n 'extraordinari': 398,\n 'prescrib': 3048,\n 'penni': 399,\n 'boyish': 400,\n 'wink': 401,\n 'publish': 402,\n 'cloud': 403,\n 'unseen': 404,\n 'composit': 3022,\n 'unobserv': 406,\n 'child': 407,\n 'friend': 408,\n 'familiarli': 410,\n 'spoke': 411,\n 'tonight': 412,\n 'affectedli': 413,\n 'lament': 3232,\n 'peculiarli': 414,\n 'provid': 2462,\n 'shade': 416,\n 'honeysuckl': 418,\n 'fondli': 419,\n 'foretel': 421,\n 'throughout': 423,\n 'against': 424,\n 'copi': 1842,\n 'destin': 3086,\n 'worthless': 2614,\n 'laudabl': 428,\n 'bustl': 429,\n 'pull': 1284,\n 'send': 431,\n 'chapter': 432,\n 'residu': 433,\n 'ball': 434,\n 'numer': 435,\n 'bring': 3025,\n 'bonnet': 437,\n 'sedul': 438,\n 'exclaim': 439,\n 'properti': 440,\n 'talker': 441,\n 'sweetmeat': 442,\n 'oppress': 443,\n 'annum': 444,\n 'basi': 445,\n 'friendli': 446,\n 'happi': 447,\n 'enrag': 449,\n 'hauteur': 450,\n 'gracious': 451,\n 'flower': 452,\n 'check': 453,\n 'proper': 454,\n 'sadli': 455,\n 'ordinari': 456,\n 'bonomi': 457,\n 'michaelma': 458,\n 'parri': 459,\n 'mourn': 460,\n 'warehous': 461,\n 'gibson': 462,\n 'describ': 463,\n 'station': 464,\n 'natur': 465,\n 'seven': 466,\n 'indispens': 467,\n 'conquest': 468,\n 'contradictori': 469,\n 'awaken': 470,\n 'unreserv': 471,\n 'night': 472,\n 'marlborough': 473,\n 'alien': 474,\n 'rider': 475,\n 'counti': 476,\n 'elud': 477,\n 'caricatur': 478,\n 'fault': 2384,\n 'farthest': 479,\n 'singli': 480,\n 'help': 481,\n 'hartshorn': 482,\n 'steadi': 483,\n 'oddest': 484,\n 'smooth': 3399,\n 'foreseen': 485,\n 'happier': 486,\n 'suffici': 487,\n 'sudden': 488,\n 'lean': 489,\n 'abil': 76,\n 'wretch': 1851,\n 'forego': 492,\n 'retard': 493,\n 'cun': 494,\n 'glori': 495,\n 'gross': 496,\n 'becam': 498,\n 'difficulti': 499,\n 'pretext': 500,\n 'dawn': 501,\n 'happiest': 502,\n 'audac': 503,\n 'discoveri': 504,\n 'avignon': 505,\n 'acquisit': 506,\n 'ensur': 507,\n 'dissembl': 2562,\n 'confus': 508,\n 'wind': 509,\n 'neglect': 510,\n 'hesit': 511,\n 'pasturag': 512,\n 'superintend': 513,\n 'ornament': 514,\n 'exclus': 1155,\n 'unfair': 515,\n 'oliv': 516,\n 'young': 517,\n 'represent': 518,\n 'among': 519,\n 'crimson': 520,\n 'alter': 521,\n 'visibl': 522,\n 'talk': 523,\n 'part': 524,\n 'storm': 525,\n 'insist': 526,\n 'specimen': 527,\n 'dairi': 528,\n 'water': 529,\n 'morton': 530,\n 'purpos': 531,\n 'root': 532,\n 'maladi': 533,\n 'stuff': 534,\n 'untri': 1856,\n 'succour': 1494,\n 'exig': 2789,\n 'alreadi': 537,\n 'consist': 538,\n 'sallow': 539,\n 'togeth': 540,\n 'unpardon': 541,\n 'columella': 542,\n 'unresist': 3210,\n 'scratch': 543,\n 'guilt': 544,\n 'faultless': 545,\n 'object': 546,\n 'unmov': 547,\n 'sad': 548,\n 'modesti': 1310,\n 'longstapl': 821,\n 'coach': 551,\n 'read': 552,\n 'minut': 553,\n 'alacr': 554,\n 'farther': 556,\n 'loos': 557,\n 'repuls': 558,\n 'unnot': 559,\n 'compet': 560,\n 'compound': 561,\n 'shill': 884,\n 'rectori': 563,\n 'repar': 3151,\n 'gener': 564,\n 'hindranc': 565,\n 'vow': 566,\n 'matern': 567,\n 'drain': 568,\n 'neighbourhood': 569,\n 'insincer': 570,\n 'toilet': 571,\n 'clever': 572,\n 'sheath': 573,\n 'mansion': 574,\n 'reserv': 575,\n 'whither': 576,\n 'contrit': 577,\n 'meadow': 578,\n 'sport': 579,\n 'handsomest': 580,\n 'adapt': 581,\n 'queen': 582,\n 'decis': 3488,\n 'suggest': 583,\n 'occur': 585,\n 'writer': 586,\n 'symptom': 587,\n 'poorli': 588,\n 'stupifi': 589,\n 'dissimilar': 591,\n 'awhil': 1866,\n 'epicur': 593,\n 'accordingli': 157,\n 'class': 595,\n 'verac': 596,\n 'vulgar': 597,\n 'noisier': 598,\n 'becaus': 1868,\n 'unreason': 600,\n 'harmless': 601,\n 'preciou': 602,\n 'prejudic': 1869,\n 'depress': 604,\n 'droll': 606,\n 'anybodi': 607,\n 'fascin': 3543,\n 'heal': 253,\n 'brave': 610,\n 'carelessli': 611,\n 'disagre': 612,\n 'regard': 204,\n 'prettiest': 615,\n 'indecor': 616,\n 'good': 617,\n 'nest': 618,\n 'children': 619,\n 'ach': 620,\n 'gentl': 621,\n 'canvass': 622,\n 'entrust': 623,\n 'retain': 624,\n 'absurd': 625,\n 'nicest': 626,\n 'difficult': 627,\n 'modestest': 2466,\n 'somerset': 2154,\n 'submit': 384,\n 'impolit': 3076,\n 'eleg': 631,\n 'fervent': 632,\n 'bewitch': 633,\n 'secur': 634,\n 'anxiou': 635,\n 'solemnli': 636,\n 'confin': 1807,\n 'seal': 637,\n 'distress': 3059,\n 'mortifi': 639,\n 'inquisit': 640,\n 'sensit': 641,\n 'acknowledg': 642,\n 'point': 643,\n 'opinion': 3061,\n 'supper': 645,\n 'wood': 646,\n 'trembl': 648,\n 'exet': 649,\n 'shelter': 1514,\n 'coven': 650,\n 'disinherit': 651,\n 'owner': 652,\n 'conform': 653,\n 'halv': 654,\n 'dwelt': 655,\n 'dessert': 656,\n 'propos': 657,\n 'experi': 658,\n 'spare': 659,\n 'stubborn': 660,\n 'christma': 661,\n 'master': 662,\n 'forbidden': 1538,\n 'tremour': 663,\n 'pillow': 664,\n 'wast': 665,\n 'voic': 666,\n 'unanim': 667,\n 'meet': 668,\n 'vehem': 669,\n 'seclud': 670,\n 'perfect': 671,\n 'austen': 672,\n 'persever': 2165,\n 'offend': 673,\n 'loung': 3465,\n 'limit': 674,\n 'attribut': 676,\n 'unconnect': 677,\n 'end': 678,\n 'wittic': 679,\n 'complic': 680,\n 'stock': 681,\n 'certain': 682,\n 'diminut': 683,\n 'fifteen': 684,\n 'uncheer': 685,\n 'expel': 686,\n 'inquiri': 687,\n 'disadvantag': 688,\n 'forti': 689,\n 'septemb': 690,\n 'thoughtless': 691,\n 'disclosur': 692,\n 'decre': 378,\n 'pleasantest': 1890,\n 'involuntarili': 695,\n 'workmen': 3459,\n 'scarc': 696,\n 'begun': 698,\n 'thickli': 2482,\n 'approach': 700,\n 'yearli': 701,\n 'reconsid': 702,\n 'auditor': 2483,\n 'compris': 704,\n 'maintain': 901,\n 'mechan': 706,\n 'ignor': 708,\n 'picturesqu': 709,\n 'independ': 922,\n 'hunt': 711,\n 'fallen': 712,\n 'wrong': 713,\n 'flush': 714,\n 'dirti': 715,\n 'lesson': 717,\n 'donavan': 718,\n 'rapidli': 719,\n 'white': 1280,\n 'demeanor': 721,\n 'willing': 722,\n 'utmost': 723,\n 'grievou': 724,\n 'earthli': 1014,\n 'openli': 726,\n 'reflect': 1022,\n 'wrote': 728,\n 'consciou': 729,\n 'spontan': 730,\n 'introduct': 731,\n 'mistaken': 1060,\n 'digniti': 733,\n 'higher': 1081,\n 'answer': 735,\n 'surfac': 1172,\n 'destini': 736,\n 'irksom': 737,\n 'penit': 738,\n 'lombardi': 739,\n 'apprehend': 740,\n 'genteelli': 741,\n 'creat': 1115,\n 'imprint': 2490,\n 'discuss': 743,\n 'plan': 744,\n 'thought': 746,\n 'tempor': 1151,\n 'polish': 748,\n 'debt': 749,\n 'twilight': 750,\n 'assign': 751,\n 'advanc': 1178,\n 'kindli': 753,\n 'spoilt': 754,\n 'unsuspect': 755,\n 'allus': 756,\n 'lengthen': 757,\n 'composur': 758,\n 'sooner': 1205,\n 'hastili': 760,\n 'singl': 761,\n 'strikingli': 762,\n 'properli': 763,\n 'nuncheon': 764,\n 'chatti': 1460,\n 'light': 3083,\n 'stage': 1241,\n 'smoke': 767,\n 'gown': 768,\n 'irresist': 770,\n 'energi': 771,\n 'dream': 1363,\n 'forgot': 774,\n 'unequ': 775,\n 'spend': 776,\n 'pinch': 1293,\n 'irregular': 778,\n 'richardson': 780,\n 'edit': 1311,\n 'signifi': 782,\n 'whomsoev': 783,\n 'impractic': 784,\n 'assembl': 785,\n 'scrupl': 786,\n 'witti': 787,\n 'eager': 3169,\n 'ferrar': 1360,\n 'tear': 789,\n 'allenham': 792,\n 'priori': 793,\n 'edtion': 794,\n 'will': 795,\n 'irrit': 796,\n 'sharp': 797,\n 'emphasi': 798,\n 'societi': 799,\n 'favour': 800,\n 'disapprov': 801,\n 'centr': 802,\n 'lane': 803,\n 'modest': 804,\n 'forgiv': 805,\n 'direct': 806,\n 'length': 807,\n 'trait': 808,\n 'ought': 809,\n 'confederaci': 810,\n 'abstract': 811,\n 'identifi': 813,\n 'instanc': 814,\n 'critic': 3430,\n 'nearer': 815,\n 'fanni': 693,\n 'alloy': 818,\n 'druri': 3075,\n 'illus': 819,\n 'necessarili': 820,\n 'event': 822,\n 'chagrin': 823,\n 'mulberri': 824,\n 'known': 825,\n 'transact': 826,\n 'effectu': 827,\n 'virul': 3552,\n 'look': 828,\n 'women': 829,\n 'listen': 830,\n 'care': 1620,\n 'paragraph': 832,\n 'fullest': 2505,\n 'ridg': 834,\n 'defer': 835,\n 'uncertainti': 836,\n 'swoon': 837,\n 'giddi': 838,\n 'tumbl': 839,\n 'usher': 840,\n 'sweetli': 841,\n 'sprung': 842,\n 'withhold': 843,\n 'languid': 3090,\n 'reconcil': 845,\n 'appar': 846,\n 'unceas': 847,\n 'dearer': 848,\n 'certainti': 849,\n 'twelv': 1911,\n 'intrins': 852,\n 'showeri': 853,\n 'argu': 3217,\n 'elect': 854,\n 'step': 855,\n 'patch': 3432,\n 'unsettl': 1784,\n 'librari': 2511,\n 'dennison': 858,\n 'undeceiv': 859,\n 'indisput': 860,\n 'counter': 3549,\n 'danc': 861,\n 'simplest': 862,\n 'compar': 863,\n 'forc': 864,\n 'christian': 865,\n 'encumb': 866,\n 'burnt': 867,\n 'altar': 868,\n 'altern': 869,\n 'enlarg': 870,\n 'sportsman': 872,\n 'elop': 873,\n 'interim': 874,\n 'pour': 3097,\n 'durat': 3323,\n 'belong': 876,\n 'firmli': 877,\n 'professedli': 1915,\n 'birth': 879,\n 'enfeebl': 2813,\n 'pervers': 880,\n 'er': 881,\n 'incur': 882,\n 'sacrif': 883,\n 'teach': 1960,\n 'unkind': 1962,\n 'steal': 1967,\n 'concis': 888,\n 'abrupt': 889,\n 'secret': 890,\n 'plump': 891,\n 'suppli': 2002,\n 'largest': 2517,\n 'examin': 2014,\n 'attent': 895,\n 'assert': 896,\n 'spaciou': 897,\n 'burst': 898,\n 'show': 899,\n 'urg': 900,\n 'flatteri': 2056,\n 'estrang': 902,\n 'instrument': 903,\n 'sob': 904,\n 'februari': 905,\n 'chariti': 906,\n 'steadfast': 907,\n 'worship': 908,\n 'basket': 2099,\n 'refin': 910,\n 'plum': 911,\n 'quicken': 912,\n 'cottag': 913,\n 'handl': 914,\n 'local': 1919,\n 'grandmoth': 916,\n 'unexhilar': 917,\n 'astray': 3470,\n 'toler': 919,\n 'altogeth': 1603,\n 'miracl': 921,\n 'sanderson': 710,\n 'school': 923,\n 'evinc': 924,\n 'tendenc': 2168,\n 'reciproc': 926,\n 'anxieti': 148,\n 'confess': 928,\n 'monopol': 929,\n 'smitten': 930,\n 'marmalad': 931,\n 'smith': 932,\n 'solicitud': 933,\n 'daili': 934,\n 'procur': 935,\n 'honour': 936,\n 'distrust': 2231,\n 'driven': 2217,\n 'apiec': 939,\n 'felicit': 940,\n 'adjoin': 941,\n 'prudent': 1318,\n 'scissor': 943,\n 'circuit': 945,\n 'needl': 946,\n 'elucid': 947,\n 'cool': 948,\n 'deem': 949,\n 'nineteen': 950,\n 'invalid': 951,\n 'rival': 952,\n 'accent': 954,\n 'kiss': 956,\n 'willoughbi': 957,\n 'commend': 958,\n 'tipto': 959,\n 'sparkl': 960,\n 'cramp': 961,\n 'westerli': 962,\n 'abat': 1072,\n 'ungenteel': 964,\n 'reward': 2403,\n 'howev': 966,\n 'attempt': 967,\n 'along': 2424,\n 'doubtingli': 969,\n 'disagr': 970,\n 'sigh': 971,\n 'directli': 2447,\n 'jacket': 3113,\n 'lie': 976,\n 'perpetu': 977,\n 'vindic': 978,\n 'exceedingli': 979,\n 'hearer': 980,\n 'specul': 981,\n 'retreat': 2508,\n 'imaginari': 983,\n 'slightest': 984,\n 'street': 986,\n 'chief': 2796,\n 'doubli': 987,\n 'composedli': 988,\n 'mind': 2532,\n 'gradual': 990,\n 'beauti': 991,\n 'chiefli': 993,\n 'captiv': 994,\n 'grief': 995,\n 'expens': 996,\n 'between': 997,\n 'greatest': 998,\n 'prove': 162,\n 'obey': 163,\n ...}"
                    }, 
                    "execution_count": 25, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "import gensim\nfrom gensim import corpora\n\n# Input to dictionary is a list of list\ndoc=[text2_stemmed]\ndictionary=corpora.Dictionary(doc)\n#print (dictionary) - Returns unique tokens\n\n#use dictionary.token2id to obtain a dict object which contains all the mappings\ntoken_to_id=dictionary.token2id\ntoken_to_id"
        }, 
        {
            "source": "## Converting document into a Vector\n\nwe use the function doc2bow to convert doc to another list which we call vec (to indicate that this is a vector).Here bow\nstands for bag of words, meaning that the order of the words in the original document is ignored and we\ntreat a document as a bag of words without any order.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 26, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(0, 3),\n (1, 1),\n (2, 1),\n (3, 13),\n (4, 2),\n (5, 5),\n (6, 25),\n (7, 1),\n (8, 1),\n (9, 1),\n (10, 4),\n (11, 15),\n (12, 1),\n (13, 13),\n (14, 18),\n (15, 1),\n (16, 6),\n (17, 1),\n (18, 3),\n (19, 1),\n (20, 6),\n (21, 1),\n (22, 5),\n (23, 144),\n (24, 2),\n (25, 1),\n (26, 4),\n (27, 12),\n (28, 32),\n (29, 1),\n (30, 1),\n (31, 3),\n (32, 3),\n (33, 22),\n (34, 1),\n (35, 1),\n (36, 2),\n (37, 46),\n (38, 8),\n (39, 1),\n (40, 3),\n (41, 3),\n (42, 6),\n (43, 29),\n (44, 39),\n (45, 9),\n (46, 1),\n (47, 1),\n (48, 6),\n (49, 1),\n (50, 1),\n (51, 12),\n (52, 30),\n (53, 14),\n (54, 9),\n (55, 1),\n (56, 21),\n (57, 16),\n (58, 2),\n (59, 87),\n (60, 3),\n (61, 13),\n (62, 1),\n (63, 20),\n (64, 1),\n (65, 6),\n (66, 3),\n (67, 44),\n (68, 1),\n (69, 1),\n (70, 2),\n (71, 36),\n (72, 1),\n (73, 4),\n (74, 6),\n (75, 65),\n (76, 12),\n (77, 3),\n (78, 4),\n (79, 1),\n (80, 1),\n (81, 15),\n (82, 9),\n (83, 32),\n (84, 1),\n (85, 6),\n (86, 74),\n (87, 1),\n (88, 34),\n (89, 33),\n (90, 1),\n (91, 53),\n (92, 1),\n (93, 31),\n (94, 30),\n (95, 4),\n (96, 2),\n (97, 4),\n (98, 3),\n (99, 4),\n (100, 2),\n (101, 75),\n (102, 1),\n (103, 3),\n (104, 2),\n (105, 89),\n (106, 1),\n (107, 19),\n (108, 8),\n (109, 1),\n (110, 5),\n (111, 67),\n (112, 10),\n (113, 2),\n (114, 3),\n (115, 2),\n (116, 2),\n (117, 2),\n (118, 3),\n (119, 102),\n (120, 1),\n (121, 2),\n (122, 4),\n (123, 6),\n (124, 2),\n (125, 5),\n (126, 32),\n (127, 32),\n (128, 1),\n (129, 8),\n (130, 1),\n (131, 11),\n (132, 2),\n (133, 1),\n (134, 2),\n (135, 1),\n (136, 1),\n (137, 1),\n (138, 1),\n (139, 2),\n (140, 5),\n (141, 2),\n (142, 1),\n (143, 3),\n (144, 1),\n (145, 11),\n (146, 93),\n (147, 1),\n (148, 18),\n (149, 2),\n (150, 1),\n (151, 1),\n (152, 2),\n (153, 1),\n (154, 63),\n (155, 30),\n (156, 9),\n (157, 3),\n (158, 5),\n (159, 1),\n (160, 1),\n (161, 1),\n (162, 20),\n (163, 1),\n (164, 8),\n (165, 1),\n (166, 1),\n (167, 2),\n (168, 1),\n (169, 113),\n (170, 12),\n (171, 7),\n (172, 1),\n (173, 160),\n (174, 6),\n (175, 1),\n (176, 1),\n (177, 1),\n (178, 3),\n (179, 9),\n (180, 6),\n (181, 1),\n (182, 1),\n (183, 1),\n (184, 19),\n (185, 9),\n (186, 2),\n (187, 1),\n (188, 1),\n (189, 62),\n (190, 3),\n (191, 3),\n (192, 5),\n (193, 45),\n (194, 37),\n (195, 2),\n (196, 1),\n (197, 4),\n (198, 2),\n (199, 1),\n (200, 1),\n (201, 1),\n (202, 2),\n (203, 17),\n (204, 62),\n (205, 1),\n (206, 2),\n (207, 1),\n (208, 16),\n (209, 1),\n (210, 20),\n (211, 4),\n (212, 21),\n (213, 2),\n (214, 4),\n (215, 31),\n (216, 1),\n (217, 7),\n (218, 6),\n (219, 9),\n (220, 7),\n (221, 12),\n (222, 40),\n (223, 48),\n (224, 1),\n (225, 2),\n (226, 1),\n (227, 6),\n (228, 2),\n (229, 8),\n (230, 2),\n (231, 10),\n (232, 21),\n (233, 10),\n (234, 64),\n (235, 1),\n (236, 1),\n (237, 1),\n (238, 1),\n (239, 1),\n (240, 1),\n (241, 1),\n (242, 1),\n (243, 7),\n (244, 14),\n (245, 1),\n (246, 23),\n (247, 6),\n (248, 1),\n (249, 4),\n (250, 1),\n (251, 18),\n (252, 3),\n (253, 1),\n (254, 1),\n (255, 6),\n (256, 1),\n (257, 3),\n (258, 11),\n (259, 1),\n (260, 17),\n (261, 35),\n (262, 6),\n (263, 2),\n (264, 34),\n (265, 1),\n (266, 1),\n (267, 39),\n (268, 6),\n (269, 2),\n (270, 12),\n (271, 4),\n (272, 377),\n (273, 1),\n (274, 17),\n (275, 1),\n (276, 5),\n (277, 1),\n (278, 5),\n (279, 3),\n (280, 33),\n (281, 1),\n (282, 6),\n (283, 4),\n (284, 121),\n (285, 1),\n (286, 23),\n (287, 3),\n (288, 4),\n (289, 1),\n (290, 1),\n (291, 6),\n (292, 4),\n (293, 3),\n (294, 4),\n (295, 2),\n (296, 1),\n (297, 2),\n (298, 10),\n (299, 2),\n (300, 1),\n (301, 75),\n (302, 4),\n (303, 2),\n (304, 1),\n (305, 1),\n (306, 6),\n (307, 1),\n (308, 8),\n (309, 8),\n (310, 4),\n (311, 2),\n (312, 79),\n (313, 2),\n (314, 4),\n (315, 1),\n (316, 4),\n (317, 3),\n (318, 1),\n (319, 1),\n (320, 3),\n (321, 8),\n (322, 1),\n (323, 52),\n (324, 261),\n (325, 2),\n (326, 1),\n (327, 1),\n (328, 2),\n (329, 1),\n (330, 17),\n (331, 1),\n (332, 1),\n (333, 37),\n (334, 11),\n (335, 1),\n (336, 1),\n (337, 3),\n (338, 1),\n (339, 3),\n (340, 1),\n (341, 12),\n (342, 8),\n (343, 13),\n (344, 8),\n (345, 5),\n (346, 3),\n (347, 1),\n (348, 7),\n (349, 1),\n (350, 9),\n (351, 1),\n (352, 1),\n (353, 1),\n (354, 1),\n (355, 1),\n (356, 9),\n (357, 1),\n (358, 1),\n (359, 8),\n (360, 21),\n (361, 10),\n (362, 6),\n (363, 4),\n (364, 2),\n (365, 5),\n (366, 1),\n (367, 23),\n (368, 22),\n (369, 18),\n (370, 1),\n (371, 2),\n (372, 1),\n (373, 1),\n (374, 1),\n (375, 1),\n (376, 1),\n (377, 9),\n (378, 1),\n (379, 2),\n (380, 1),\n (381, 1),\n (382, 1),\n (383, 1),\n (384, 17),\n (385, 1),\n (386, 2),\n (387, 1),\n (388, 1),\n (389, 29),\n (390, 6),\n (391, 36),\n (392, 4),\n (393, 1),\n (394, 4),\n (395, 6),\n (396, 4),\n (397, 46),\n (398, 9),\n (399, 2),\n (400, 1),\n (401, 1),\n (402, 1),\n (403, 4),\n (404, 1),\n (405, 18),\n (406, 1),\n (407, 41),\n (408, 117),\n (409, 2),\n (410, 2),\n (411, 35),\n (412, 3),\n (413, 2),\n (414, 2),\n (415, 1),\n (416, 4),\n (417, 2),\n (418, 1),\n (419, 4),\n (420, 6),\n (421, 1),\n (422, 3),\n (423, 1),\n (424, 1),\n (425, 1),\n (426, 39),\n (427, 1),\n (428, 1),\n (429, 3),\n (430, 1),\n (431, 14),\n (432, 50),\n (433, 1),\n (434, 3),\n (435, 4),\n (436, 1),\n (437, 1),\n (438, 1),\n (439, 14),\n (440, 9),\n (441, 1),\n (442, 1),\n (443, 10),\n (444, 1),\n (445, 1),\n (446, 9),\n (447, 166),\n (448, 3),\n (449, 1),\n (450, 1),\n (451, 3),\n (452, 2),\n (453, 12),\n (454, 24),\n (455, 3),\n (456, 5),\n (457, 1),\n (458, 4),\n (459, 1),\n (460, 1),\n (461, 1),\n (462, 1),\n (463, 16),\n (464, 2),\n (465, 87),\n (466, 14),\n (467, 5),\n (468, 6),\n (469, 1),\n (470, 6),\n (471, 6),\n (472, 37),\n (473, 3),\n (474, 1),\n (475, 2),\n (476, 4),\n (477, 1),\n (478, 1),\n (479, 2),\n (480, 1),\n (481, 2),\n (482, 1),\n (483, 8),\n (484, 1),\n (485, 2),\n (486, 7),\n (487, 11),\n (488, 12),\n (489, 8),\n (490, 8),\n (491, 36),\n (492, 2),\n (493, 1),\n (494, 3),\n (495, 1),\n (496, 1),\n (497, 1),\n (498, 17),\n (499, 36),\n (500, 1),\n (501, 1),\n (502, 6),\n (503, 1),\n (504, 7),\n (505, 2),\n (506, 3),\n (507, 3),\n (508, 8),\n (509, 4),\n (510, 6),\n (511, 8),\n (512, 1),\n (513, 1),\n (514, 3),\n (515, 1),\n (516, 1),\n (517, 103),\n (518, 3),\n (519, 21),\n (520, 2),\n (521, 26),\n (522, 2),\n (523, 62),\n (524, 28),\n (525, 1),\n (526, 5),\n (527, 2),\n (528, 1),\n (529, 5),\n (530, 17),\n (531, 22),\n (532, 1),\n (533, 1),\n (534, 2),\n (535, 26),\n (536, 2),\n (537, 49),\n (538, 5),\n (539, 1),\n (540, 73),\n (541, 5),\n (542, 1),\n (543, 2),\n (544, 7),\n (545, 3),\n (546, 68),\n (547, 1),\n (548, 2),\n (549, 8),\n (550, 1),\n (551, 4),\n (552, 7),\n (553, 63),\n (554, 2),\n (555, 5),\n (556, 49),\n (557, 1),\n (558, 2),\n (559, 1),\n (560, 3),\n (561, 2),\n (562, 1),\n (563, 2),\n (564, 72),\n (565, 2),\n (566, 1),\n (567, 2),\n (568, 2),\n (569, 10),\n (570, 3),\n (571, 1),\n (572, 5),\n (573, 1),\n (574, 5),\n (575, 16),\n (576, 5),\n (577, 2),\n (578, 1),\n (579, 4),\n (580, 2),\n (581, 1),\n (582, 1),\n (583, 11),\n (584, 263),\n (585, 22),\n (586, 3),\n (587, 7),\n (588, 1),\n (589, 1),\n (590, 3),\n (591, 1),\n (592, 1),\n (593, 1),\n (594, 1),\n (595, 1),\n (596, 2),\n (597, 4),\n (598, 1),\n (599, 26),\n (600, 3),\n (601, 1),\n (602, 4),\n (603, 16),\n (604, 1),\n (605, 1),\n (606, 7),\n (607, 11),\n (608, 17),\n (609, 2),\n (610, 1),\n (611, 1),\n (612, 2),\n (613, 64),\n (614, 1),\n (615, 1),\n (616, 1),\n (617, 8),\n (618, 1),\n (619, 33),\n (620, 5),\n (621, 13),\n (622, 1),\n (623, 1),\n (624, 4),\n (625, 6),\n (626, 3),\n (627, 5),\n (628, 7),\n (629, 5),\n (630, 19),\n (631, 28),\n (632, 3),\n (633, 2),\n (634, 31),\n (635, 29),\n (636, 2),\n (637, 1),\n (638, 1),\n (639, 6),\n (640, 2),\n (641, 1),\n (642, 36),\n (643, 51),\n (644, 3),\n (645, 4),\n (646, 6),\n (647, 57),\n (648, 2),\n (649, 15),\n (650, 1),\n (651, 1),\n (652, 3),\n (653, 3),\n (654, 1),\n (655, 2),\n (656, 1),\n (657, 12),\n (658, 4),\n (659, 23),\n (660, 3),\n (661, 2),\n (662, 1),\n (663, 1),\n (664, 1),\n (665, 2),\n (666, 53),\n (667, 4),\n (668, 27),\n (669, 2),\n (670, 2),\n (671, 11),\n (672, 1),\n (673, 19),\n (674, 4),\n (675, 7),\n (676, 11),\n (677, 1),\n (678, 10),\n (679, 1),\n (680, 1),\n (681, 4),\n (682, 32),\n (683, 2),\n (684, 5),\n (685, 2),\n (686, 1),\n (687, 31),\n (688, 3),\n (689, 4),\n (690, 2),\n (691, 1),\n (692, 1),\n (693, 48),\n (694, 11),\n (695, 2),\n (696, 25),\n (697, 2),\n (698, 7),\n (699, 5),\n (700, 10),\n (701, 3),\n (702, 1),\n (703, 1),\n (704, 2),\n (705, 5),\n (706, 1),\n (707, 11),\n (708, 17),\n (709, 7),\n (710, 1),\n (711, 2),\n (712, 2),\n (713, 24),\n (714, 1),\n (715, 4),\n (716, 3),\n (717, 1),\n (718, 5),\n (719, 3),\n (720, 11),\n (721, 1),\n (722, 2),\n (723, 15),\n (724, 2),\n (725, 2),\n (726, 11),\n (727, 4),\n (728, 12),\n (729, 9),\n (730, 1),\n (731, 3),\n (732, 4),\n (733, 4),\n (734, 1),\n (735, 62),\n (736, 1),\n (737, 1),\n (738, 3),\n (739, 1),\n (740, 1),\n (741, 1),\n (742, 1),\n (743, 5),\n (744, 6),\n (745, 18),\n (746, 151),\n (747, 1),\n (748, 1),\n (749, 1),\n (750, 2),\n (751, 2),\n (752, 6),\n (753, 2),\n (754, 1),\n (755, 1),\n (756, 1),\n (757, 3),\n (758, 17),\n (759, 1),\n (760, 12),\n (761, 8),\n (762, 2),\n (763, 5),\n (764, 1),\n (765, 2),\n (766, 1),\n (767, 1),\n (768, 1),\n (769, 4),\n (770, 3),\n (771, 4),\n (772, 78),\n (773, 9),\n (774, 12),\n (775, 1),\n (776, 13),\n (777, 1),\n (778, 1),\n (779, 44),\n (780, 4),\n (781, 1),\n (782, 7),\n (783, 1),\n (784, 1),\n (785, 3),\n (786, 8),\n (787, 1),\n (788, 5),\n (789, 23),\n (790, 2),\n (791, 1),\n (792, 18),\n (793, 1),\n (794, 1),\n (795, 7),\n (796, 7),\n (797, 4),\n (798, 1),\n (799, 24),\n (800, 33),\n (801, 5),\n (802, 2),\n (803, 2),\n (804, 2),\n (805, 17),\n (806, 25),\n (807, 13),\n (808, 1),\n (809, 36),\n (810, 1),\n (811, 2),\n (812, 2),\n (813, 1),\n (814, 7),\n (815, 7),\n (816, 2),\n (817, 2),\n (818, 1),\n (819, 1),\n (820, 3),\n (821, 8),\n (822, 30),\n (823, 1),\n (824, 2),\n (825, 60),\n (826, 2),\n (827, 3),\n (828, 99),\n (829, 11),\n (830, 24),\n (831, 5),\n (832, 2),\n (833, 3),\n (834, 1),\n (835, 6),\n (836, 2),\n (837, 1),\n (838, 1),\n (839, 3),\n (840, 1),\n (841, 1),\n (842, 5),\n (843, 2),\n (844, 8),\n (845, 5),\n (846, 6),\n (847, 6),\n (848, 6),\n (849, 5),\n (850, 8),\n (851, 7),\n (852, 1),\n (853, 2),\n (854, 2),\n (855, 6),\n (856, 1),\n (857, 3),\n (858, 1),\n (859, 1),\n (860, 1),\n (861, 20),\n (862, 1),\n (863, 10),\n (864, 39),\n (865, 2),\n (866, 1),\n (867, 1),\n (868, 2),\n (869, 3),\n (870, 2),\n (871, 2),\n (872, 3),\n (873, 1),\n (874, 2),\n (875, 1),\n (876, 10),\n (877, 4),\n (878, 31),\n (879, 1),\n (880, 2),\n (881, 3),\n (882, 4),\n (883, 4),\n (884, 2),\n (885, 1),\n (886, 3),\n (887, 3),\n (888, 1),\n (889, 1),\n (890, 21),\n (891, 1),\n (892, 10),\n (893, 2),\n (894, 35),\n (895, 76),\n (896, 10),\n (897, 2),\n (898, 14),\n (899, 4),\n (900, 12),\n (901, 6),\n (902, 1),\n (903, 7),\n (904, 1),\n (905, 6),\n (906, 2),\n (907, 1),\n (908, 1),\n (909, 16),\n (910, 2),\n (911, 1),\n (912, 1),\n (913, 63),\n (914, 1),\n (915, 29),\n (916, 1),\n (917, 1),\n (918, 2),\n (919, 13),\n (920, 68),\n (921, 1),\n (922, 15),\n (923, 6),\n (924, 1),\n (925, 7),\n (926, 2),\n (927, 6),\n (928, 22),\n (929, 1),\n (930, 1),\n (931, 1),\n (932, 18),\n (933, 13),\n (934, 4),\n (935, 20),\n (936, 28),\n (937, 5),\n (938, 2),\n (939, 1),\n (940, 1),\n (941, 2),\n (942, 107),\n (943, 3),\n (944, 2),\n (945, 1),\n (946, 2),\n (947, 1),\n (948, 3),\n (949, 1),\n (950, 5),\n (951, 1),\n (952, 5),\n (953, 1),\n (954, 3),\n (955, 15),\n (956, 4),\n (957, 219),\n (958, 9),\n (959, 1),\n (960, 2),\n (961, 1),\n (962, 1),\n (963, 1),\n (964, 1),\n (965, 9),\n (966, 155),\n (967, 39),\n (968, 3),\n (969, 1),\n (970, 2),\n (971, 5),\n (972, 1),\n (973, 3),\n (974, 3),\n (975, 2),\n (976, 3),\n (977, 2),\n (978, 4),\n (979, 26),\n (980, 1),\n (981, 2),\n (982, 1),\n (983, 3),\n (984, 3),\n (985, 1),\n (986, 51),\n (987, 1),\n (988, 1),\n (989, 14),\n (990, 2),\n (991, 39),\n (992, 1),\n (993, 7),\n (994, 4),\n (995, 12),\n (996, 26),\n (997, 2),\n (998, 28),\n (999, 9),\n ...]"
                    }, 
                    "execution_count": 26, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "vec=dictionary.doc2bow(text2_stemmed)\nvec"
        }, 
        {
            "execution_count": 27, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "6\n20\n"
                }
            ], 
            "cell_type": "code", 
            "source": "# Checking the vector indeed represents the doc\nprint (text2_stemmed.count('fall'))\nprint (dictionary.token2id['fall'])"
        }, 
        {
            "source": "## Computing similarities between documents\n\nGensim has built-in functions to compute cosine similarities between documents.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 28, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "<gensim.similarities.docsim.SparseMatrixSimilarity at 0x7feb5d132ba8>"
                    }, 
                    "execution_count": 28, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "# Define a corpus with 2 documents represented as sparse vectors\n\nmycorpus=[[(0,1),(1,1),(2,1)],[(1,2),(3,1)]]\nfrom gensim import similarities\n\n# Build an index for efficient computation. In below example, 4 is the dimension of the sparse matrix\nindex=similarities.SparseMatrixSimilarity(mycorpus,4)\nindex"
        }, 
        {
            "execution_count": 29, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[(0, 0.57735026), (1, 0.0)]\n"
                }
            ], 
            "cell_type": "code", 
            "source": "#To compute similarity of a new document, \n\ntest_doc=[(0,1)]\nsims=index[test_doc]\nprint (list(enumerate(sims)))"
        }, 
        {
            "source": "#### Results above shows test_doc has 0.57 similarity to 1st document in mycorpus and has 0.0 similarity to 2nd document in mycorpus", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## TF-IDFWeighting\nWe can use Gensim to help compute inverse document frequencies (IDFs) and use them to re-assign weights\nto document vectors. To do this, we need to import models from Gensim first.\n\ntfidf object automatically normalizes the vectors when it transforms them. When a vector is normalized, the value of each entry is\ndivided by the norm of this vector, such that the norm of the new vector is exactly 1.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 30, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(0, 1.0)]"
                    }, 
                    "execution_count": 30, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "from gensim import models\n#tfidf object is created on entire corpus\ntfidf=models.TfidfModel(mycorpus)\n#test new documents using the tfidf object\ntfidf[test_doc]"
        }, 
        {
            "source": "## Document Retrieval\nNext thing is to see how cosine similarity with TFIDF weighting can be used for the task of document retrieval.\n\nGiven a query represented as a set of words,the goal of document retrieval is the find a list of documents from a corpus that are the most relevant to the\nquery.\n\nThis can be done by ranking all the documents in the corpus based on their cosine similarities to the\nquery.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 17, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Singapore can expect more rain and less haze in the coming weeks with the south-west monsoon season transitioning into inter-monsoon conditions.The inter-monsoon season typically lasts from October to November and the weather during the period is characterised by more rainfall and light and variable winds.The Meteorological Service Singapore said on Monday in an advisory that this transition signals the end of traditional dry season in the region, and the likelihood of transboundary haze affecting Singapore for the rest of the year will be low.This is because the increased rainfall will help alleviate the hotspot and haze situation in Sumatra and Kalimantan in Indonesia. <_io.TextIOWrapper name='newcorpus/1.txt' mode='w' encoding='UTF-8'>\nTrain services between Admiralty and Jurong East MRT stations will end half an hour earlier from Sunday, Nov 2, to end March next year, due to rail works.SMRT will end services at nine North-South Line stations by 12.30am on Sunday to Thursday nights, except on the eve of public holidays.The stations are: Admiralty, Woodlands, Marsiling, Kranji, Yew Tee, Choa Chu Kang, Bukit Gombak, Bukit Batok and Jurong East.From Nov 2, commuters who board trains after 11.15pm on the North-South Line are advised to plan their journey and consider alternative transport arrangements such as bus services to get to their final destination, said SMRT in a statement to remind the public on Monday. <_io.TextIOWrapper name='newcorpus/2.txt' mode='w' encoding='UTF-8'>\n"
                }
            ], 
            "cell_type": "code", 
            "source": "import sys\nfrom nltk.corpus.reader.plaintext import PlaintextCorpusReader\nstring1=\"\"\"Singapore can expect more rain and less haze in the coming weeks with the south-west monsoon season transitioning into inter-monsoon conditions.The inter-monsoon season typically lasts from October to November and the weather during the period is characterised by more rainfall and light and variable winds.The Meteorological Service Singapore said on Monday in an advisory that this transition signals the end of traditional dry season in the region, and the likelihood of transboundary haze affecting Singapore for the rest of the year will be low.This is because the increased rainfall will help alleviate the hotspot and haze situation in Sumatra and Kalimantan in Indonesia.\"\"\"\nstring2=\"\"\"Train services between Admiralty and Jurong East MRT stations will end half an hour earlier from Sunday, Nov 2, to end March next year, due to rail works.SMRT will end services at nine North-South Line stations by 12.30am on Sunday to Thursday nights, except on the eve of public holidays.The stations are: Admiralty, Woodlands, Marsiling, Kranji, Yew Tee, Choa Chu Kang, Bukit Gombak, Bukit Batok and Jurong East.From Nov 2, commuters who board trains after 11.15pm on the North-South Line are advised to plan their journey and consider alternative transport arrangements such as bus services to get to their final destination, said SMRT in a statement to remind the public on Monday.\"\"\"\ncorpus = [string1,string2]\ncorpusdir = 'newcorpus/'\nif not os.path.isdir(corpusdir):\n    os.mkdir(corpusdir)\n    \nfilename = 0\nfor text in corpus:\n    filename+=1\n    with open(corpusdir+str(filename)+'.txt','w') as fout:\n        print (text,fout)\n        fout.write(text)\n\n\n"
        }, 
        {
            "execution_count": 52, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[['Singapore', 'can', 'expect', 'more', 'rain', 'and', ...], ['Train', 'services', 'between', 'Admiralty', 'and', ...]]\n<class 'list'>\n[[(0, 1), (1, 1), (2, 1), (3, 3), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 4), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 3), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 3), (32, 1), (33, 1), (34, 3), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 3), (49, 1), (50, 1), (51, 1)], [(2, 1), (3, 2), (7, 1), (10, 13), (15, 6), (37, 1), (39, 2), (47, 3), (50, 3), (52, 1), (53, 1), (54, 2), (55, 2), (56, 2), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 2), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 1), (77, 2), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 3), (85, 1), (86, 1), (87, 2), (88, 1), (89, 1), (90, 1), (91, 2), (92, 1), (93, 2), (94, 1), (95, 2), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 2), (106, 1), (107, 1), (108, 2), (109, 1)]]\nTfidfModel(num_docs=2, num_nnz=119)\n"
                }
            ], 
            "cell_type": "code", 
            "source": "newcorpus = PlaintextCorpusReader('/gpfs/global_fs01/sym_shared/YPProdSpark/user/sfbc-20c2d955c74628-3c618564d05f/notebook/work/newcorpus', '.*')\nfids= (newcorpus.fileids())\ndocs=[newcorpus.words(f) for f in fids]\nprint (docs)\n# Change words to lowercase\ndocs=tolower(docs)\n#print(docs)\n#Remove stop words\ndocs=removestop(docs)\n#Perform stemming\ndocs=stemwords(docs)\n\n#Create dictionary\ndictionary=fetchdictionary(docs)\ntoken_to_id=dictionary.token2id\n#Convert to vector\nprint (type(docs))\nvecs=convertToVec(docs,dictionary)\nprint (vecs)\n#Build index for finding similarity\nindex=buildindex(vecs)\n#print(index)\n\ntdif=createtdif(vecs)\nprint (tdif)\n"
        }, 
        {
            "execution_count": 42, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "def tolower(docs):\n    docs=[[w.lower() for w in doc] for doc in docs]\n    return docs\n    \ndef fetchdictionary(docs):\n    dictionary=corpora.Dictionary(docs)\n    return dictionary\n\ndef removestop(docs):\n    docs=[[w for w in doc if w not in stop_list] for doc in docs]\n    return docs;\n\ndef stemwords(docs):\n    docs=[[stemmer.stem(w) for w in doc] for doc in docs]\n    \n    #text2_stemmed=[stemmer.stem(w) for w in wordlist]\n    return docs;\n\ndef convertToVec(docs,dictionary):\n    vecs=[dictionary.doc2bow(doc) for doc in docs]\n    return vecs\n\ndef buildindex(docs):\n    index=similarities.SparseMatrixSimilarity(docs,110)\n    return index;\n\ndef createtdif(docs):\n    tfidf=models.TfidfModel(docs)\n    return tfidf\n"
        }, 
        {
            "execution_count": 47, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[(25, 1), (31, 1)]\n"
                }, 
                {
                    "data": {
                        "text/plain": "[(25, 0.7071067811865475), (31, 0.7071067811865475)]"
                    }, 
                    "execution_count": 47, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "#Retrive document from newcorpus based on the query\nquery1=[stemmer.stem('singapore'),stemmer.stem('indonesia')]\n# Convert query to sparse vector\nquery1_vec=dictionary.doc2bow(query1)\nprint (vect)\n# See the importance of the query in the corpus by performing TF_IDF. Both Singapore and Indonesia words appear in Doc 1 and therefore has the same IDF value.\nquery1_vec_tdif=tdif[query1_vec]\nquery1_vec_tdif"
        }, 
        {
            "execution_count": 51, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[(0, 0.26261285), (1, 0.0)]\n"
                }, 
                {
                    "data": {
                        "text/plain": "'2.txt'"
                    }, 
                    "execution_count": 51, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "# Next is to find cosine similarity between the query and all documents in the corpus.\nsims=index[query1_vec_tdif]\n\n# Sort in descending order by similarity score.\nsorted_sims=sorted(enumerate(sims),key=lambda item: -item[1])\nprint (sorted_sims[0:10])\n\n# Check the ID of the document which is similar to the query\nnewcorpus.fileids()[1]"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "source": ""
        }
    ], 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "version": "3.5.2", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "name": "python", 
            "file_extension": ".py", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21"
        }
    }
}